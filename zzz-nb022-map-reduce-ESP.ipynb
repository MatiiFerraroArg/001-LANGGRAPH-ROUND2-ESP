{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac76bd4-dd06-4edf-b2c4-e53991b5cf65",
   "metadata": {},
   "source": [
    "# Operaciones de Map-Reduce con Send\n",
    "* Las operaciones de map-reduce son esenciales para una descomposición eficiente de tareas y un procesamiento en paralelo.\n",
    "    * Map: Divide una tarea en sub-tareas más pequeñas, procesando cada sub-tarea en paralelo.\n",
    "    * Reduce: Agrega los resultados de todas las sub-tareas completadas y paralelizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314975c1-3b54-4f73-af26-aabe618a8f3a",
   "metadata": {},
   "source": [
    "## Ummm... De nuevo, ¿qué es una operación de Map-Reduce???\n",
    "\n",
    "Si no estás familiarizado con este término y te suena confuso, no te preocupes. Su verdadero significado es más fácil de entender de lo que parece por su nombre extraño.\n",
    "\n",
    "#### Map  \n",
    "Piensa en esto como **dividir** un trabajo grande en tareas más pequeñas y manejables. Cada tarea más pequeña se puede ejecutar al mismo tiempo (en **paralelo**).\n",
    "\n",
    "Ejemplo:  \n",
    "* Si tienes que contar las palabras en 100 libros, puedes dar 10 libros a 10 personas diferentes, y cada persona cuenta las palabras en los libros que le fueron asignados.\n",
    "\n",
    "#### Reduce  \n",
    "Una vez que todas las tareas más pequeñas han finalizado, los resultados se **combinan** para producir el resultado final.\n",
    "\n",
    "Ejemplo:  \n",
    "* Las 10 personas reportan sus conteos de palabras, y tú **sumas** todos los conteos para obtener el número total de palabras en los 100 libros.\n",
    "\n",
    "#### Procesamiento de Map-Reduce  \n",
    "- **Map**: Divide una tarea grande en tareas más pequeñas.  \n",
    "- **Reduce**: Combina los resultados de las tareas más pequeñas en el resultado final.\n",
    "\n",
    "Este enfoque ahorra tiempo al **distribuir el trabajo** de manera eficiente, especialmente cuando se trabaja con grandes conjuntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88643a1-aaed-429c-a181-3c448f202525",
   "metadata": {},
   "source": [
    "## ¿Y cómo se realiza una operación de Map-Reduce en LangGraph?\n",
    "\n",
    "En LangGraph, el patrón de map-reduce es un método para manejar tareas dividiéndolas en sub-tareas más pequeñas, procesando estas sub-tareas simultáneamente y luego combinando los resultados. Este enfoque es particularmente útil cuando el número de sub-tareas no se conoce de antemano.\n",
    "\n",
    "#### ¿Cómo funciona?\n",
    "\n",
    "1. **Mapping:** Se parte de una tarea principal que genera una lista de elementos. Por ejemplo, si se proporciona un tema general, el sistema podría producir una lista de temas relacionados.\n",
    "\n",
    "2. **Processing in Parallel:** Cada elemento de la lista se procesa de forma independiente y al mismo tiempo. Siguiendo el ejemplo, para cada tema relacionado, el sistema podría generar un chiste.\n",
    "\n",
    "3. **Reducing:** Después de que todos los elementos hayan sido procesados, los resultados se combinan o resumen. En este caso, esto podría implicar seleccionar el mejor chiste de la lista generada.\n",
    "\n",
    "#### Desafíos abordados\n",
    "\n",
    "- **Número desconocido de sub-tareas:** Dado que la cantidad de elementos (como los temas relacionados) puede no conocerse de antemano, resulta complicado configurar el flujo de trabajo previamente.\n",
    "\n",
    "- **States de input dinámicos:** Cada sub-tarea podría requerir un input diferente, lo que exige un sistema capaz de manejar inputs variables de manera dinámica.\n",
    "\n",
    "#### La solución de LangGraph: usar Send\n",
    "\n",
    "Como veremos a continuación, **LangGraph utiliza la API integrada Send para gestionar estos desafíos**. Empleando edges condicionales, la función Send puede distribuir diferentes inputs a múltiples instancias de un nodo de procesamiento. Esto significa que cada sub-tarea puede recibir un input único, permitiendo una gestión de flujo de trabajo flexible y dinámica.\n",
    "\n",
    "Este enfoque de map-reduce en LangGraph permite una descomposición eficiente de tareas y un procesamiento en paralelo, facilitando la gestión de workflows complejos donde el número de sub-tareas no está determinado previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7fdeb-2543-4280-be25-8e962552530c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e336-d054-412c-8a3f-1fbec63e1bcd",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda8d4-80cf-4b8f-9981-94edda5e9911",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 022-map-reduce.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-langgraph**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e766aa-f3e2-491f-be99-d0c6b700d47a",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df978ec5-bfd2-4167-bd33-86bc2687d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel35 = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chatModel4o = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad87382-86da-4412-848b-9f434b85d7ca",
   "metadata": {},
   "source": [
    "## Ejemplo básico de operación Map-Reduce: Una aplicación generadora de chistes\n",
    "\n",
    "#### El proceso de map-reduce  \n",
    "Diseñaremos una aplicación que realizará dos tareas:  \n",
    "* Crear un conjunto de chistes a partir de temas o subtemas relacionados con un tema dado (operación de map).  \n",
    "* Elegir el mejor chiste de la lista (operación de reduce).  \n",
    "\n",
    "#### El workflow de la aplicación será:  \n",
    "* El usuario ingresará un tema.  \n",
    "* La aplicación generará una lista de temas o subtemas relacionados con el tema ingresado.  \n",
    "* La aplicación generará un chiste para cada tema.  \n",
    "* La aplicación seleccionará el mejor chiste generado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df3329-46f1-4e5b-8c3e-03649980f5be",
   "metadata": {},
   "source": [
    "## Comencemos creando los prompts que usaremos con el LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f3e2e3-5f7f-458d-ac64-eba955b01ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Prompts we will use\n",
    "subjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\"\n",
    "joke_prompt = \"\"\"Generate a joke about {subject}\"\"\"\n",
    "best_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: \\n\\n  {jokes}\"\"\"\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8765d-3125-4a7e-8b8a-1056d6e87466",
   "metadata": {},
   "source": [
    "## Ahora definamos los schemas que establecerán el formato de Subjects, BestJoke y OverallState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b0f650-8c60-4c4c-832c-34ca67accbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Subjects(BaseModel):\n",
    "    subjects: list[str]\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int\n",
    "\n",
    "# PAY ATTENTION HERE: The `jokes` key will append the jokes.\n",
    "# See how we are using the operator.add reducer.\n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: list\n",
    "    jokes: Annotated[list, operator.add]\n",
    "    best_selected_joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d6f3a-d15a-42c4-b188-7ad403496076",
   "metadata": {},
   "source": [
    "## Ahora definiremos la función para generar subtemas (en inglés los llamaremos subjects) relacionados con el tema (topic) enviado por el usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3494781b-094f-476b-93cd-f7d9b4511b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are asking the LLM to generate subjects related with the topic\n",
    "def generate_subjects(state: OverallState):\n",
    "    prompt = subjects_prompt.format(topic=state[\"topic\"])\n",
    "    response = model.with_structured_output(Subjects).invoke(prompt)\n",
    "    return {\"subjects\": response.subjects}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6495b0d-2027-4c1d-ad89-a5b8a19d7ff6",
   "metadata": {},
   "source": [
    "## Expliquemos el código anterior en términos sencillos\n",
    "\n",
    "Aquí tienes una explicación sencilla del código anterior:\n",
    "\n",
    "1. **Propósito**: La función `generate_subjects` está diseñada para usar un LLM (referido como `model`) y generar una lista de subtemas relacionados con un tema dado.\n",
    "\n",
    "2. **Entrada**: Toma un único argumento llamado `state`, que es un objeto similar a un diccionario (`OverallState`). Este `state` contiene una key `\"topic\"` que especifica el tema principal para el cual se deben generar subtemas relacionados.\n",
    "\n",
    "3. **Pasos**:\n",
    "   - **Crear un Prompt**: La expresión `subjects_prompt.format(topic=state[\"topic\"])` toma una plantilla de prompt predefinida (`subjects_prompt`) y la rellena con el tema extraído de `state`.\n",
    "   - **Consultar el Modelo**: Se usa el LLM (`model`) para procesar este prompt. El método `with_structured_output(Subjects).invoke(prompt)` asegura que la respuesta del modelo tenga una estructura y siga un formato específico definido en `Subjects`.\n",
    "   - **Extraer Subtemas**: Se extrae el campo `subjects` de la respuesta del modelo.\n",
    "\n",
    "4. **Salida**: La función devuelve un diccionario con una única clave, `\"subjects\"`, que contiene la lista de temas generados por el modelo.\n",
    "\n",
    "#### Ejemplo:\n",
    "\n",
    "Si el tema en `state[\"topic\"]` es `\"cambio climático\"`, la función:\n",
    "1. Formatea un prompt como: \"Genera una lista de subtemas relacionados con el cambio climático.\"\n",
    "2. Envía este prompt al modelo.\n",
    "3. Recibe y procesa la respuesta del modelo (por ejemplo, `[\"Calentamiento Global\", \"Energías Renovables\", \"Huella de Carbono\"]`).\n",
    "4. Devuelve: `{\"subjects\": [\"Calentamiento Global\", \"Energías Renovables\", \"Huella de Carbono\"]}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b260e-777f-48b6-9f9e-1aacf6b44ed7",
   "metadata": {},
   "source": [
    "## Una vez que tengamos la lista de subtemas, usaremos Send para crear un chiste para cada subtema\n",
    "\n",
    "En términos simples, la función **`Send`** en LangGraph se utiliza para enviar dinámicamente datos (llamados **state**) a nodos específicos en un workflow, incluso cuando no sabes de antemano cuántos nodos necesitarás o qué datos recibirá cada uno.  \n",
    "\n",
    "#### Escenario de ejemplo:  \n",
    "Imagina que tienes un proceso donde:  \n",
    "1. **Nodo A** crea una lista de subtemas (por ejemplo, `[\"gatos\", \"perros\", \"robots\"]`).  \n",
    "2. Quieres enviar cada subtema a otro nodo (**\"generate_joke\"**) que generará un chiste sobre ese subtema.  \n",
    "\n",
    "Sin embargo, no sabes de antemano cuántos subtemas habrá.  \n",
    "\n",
    "#### ¿Qué hace `Send`?  \n",
    "- Permite crear tantas conexiones como sea necesario **sobre la marcha** (en lugar de predefinirlas).  \n",
    "- Cada conexión puede tener sus **propios datos** (cada tema en particular).  \n",
    "\n",
    "#### Paso 1: usar un edge condicional para conectar el \"Nodo A\" con \"la Magia de Send\".\n",
    "```python\n",
    "graph.add_conditional_edges(\"node_a\", continue_to_jokes)\n",
    "```\n",
    "- **Conecta el \"nodo_a\" al nodo \"generate_joke\" de manera dinámica, creando un chiste para cada subtema asociado**. \n",
    "\n",
    "#### Paso 2: \"la Magia de Send\" crea tantos nodos generate_joke en paralelo como subtemas haya generado el Nodo A.\n",
    "```python\n",
    "def continue_to_jokes(state: OverallState):\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n",
    "```\n",
    "- Esta función toma el state (por ejemplo, una lista de subtemas).  \n",
    "- Crea un objeto Send **para cada subtema, indicándole**:  \n",
    "  1. **Ir al nodo `generate_joke`**.  \n",
    "  2. **Pasar un subtema específico (por ejemplo, `{\"subject\": \"gatos\"}`) como input**.   \n",
    "\n",
    "#### Idea clave:\n",
    "**En lugar de definir conexiones fijas, `Send` permite que el workflow se adapte en tiempo de ejecución según los datos**. Esto es especialmente útil para tareas como el procesamiento de listas o lotes de datos.\n",
    "\n",
    "#### Cómo usaremos Send en nuestro ejercicio\n",
    "* Usaremos [Send](https://langchain-ai.github.io/langgraph/concepts/low_level/#send) para crear un chiste para cada subtema. **Esto puede paralelizar automáticamente la generación de chistes para cualquier cantidad de subtemas**.\n",
    "    * `generate_joke`: el nombre del nodo en el graph.\n",
    "    * `{\"subject\": s}`: el state que se enviará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f1bb3df-06a7-4145-a8b9-a7f64cf4bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "def continue_to_jokes(state: OverallState):\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a8d36-7783-4fe7-bf55-19aa25608505",
   "metadata": {},
   "source": [
    "## Explicación del código anterior en términos sencillos\n",
    "\n",
    "El código define una función llamada **`continue_to_jokes`** que toma algunos datos de input (**state**) y prepara instrucciones para enviar cada elemento de esos datos a un step específico de procesamiento.\n",
    "\n",
    "#### Desglosando el código:\n",
    "\n",
    "1. **Input: `state`**  \n",
    "   - El input, llamado **`state`**, contiene datos organizados como un diccionario.  \n",
    "   - Tiene una key llamada **\"subjects\"**, que se espera que sea una **lista** (por ejemplo, `[\"gatos\", \"perros\", \"robots\"]`).\n",
    "\n",
    "2. **Output: Lista de objetos `Send`**  \n",
    "   - Para cada subtema en la lista, la función crea un objeto **`Send`**.  \n",
    "   - Cada objeto **`Send`** especifica:  \n",
    "     1. Ir al nodo **\"generate_joke\"** (un paso en el flujo de trabajo).  \n",
    "     2. Pasar un **subtema específico** como input, por ejemplo, `{\"subject\": \"gatos\"}`.\n",
    "\n",
    "3. **Propósito**  \n",
    "   - La función se usa para **redirigir datos dinámicamente**.  \n",
    "   - Si hay 3 subtemas en la lista, crea 3 instrucciones `Send`, una para cada subtema, indicando al sistema que procese cada elemento de forma individual.\n",
    "\n",
    "#### Ejemplo:  \n",
    "Si **state[\"subjects\"] = [\"gatos\", \"perros\"]**, el output será:  \n",
    "```\n",
    "[\n",
    "    Send(\"generate_joke\", {\"subject\": \"gatos\"}),  \n",
    "    Send(\"generate_joke\", {\"subject\": \"perros\"})\n",
    "]\n",
    "```\n",
    "\n",
    "Esto permite que el worlflow maneje múltiples inputs (subtemas) en paralelo, enviando cada una al nodo **\"generate_joke\"** para su procesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fbafb8-f343-41b1-88bf-574644f6bce5",
   "metadata": {},
   "source": [
    "## ¡Ups! Pero aún no hemos definido la función creadora de chistes. Hagámoslo ahora.\n",
    "* Definiremos un nodo que generará nuestros chistes, llamado `generate_joke`.  \n",
    "* Los devolveremos como `jokes` en `OverallState`.  \n",
    "* Esta key tiene un **reducer** que combinará las lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0586732f-51bd-4971-9319-c40216598ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "# PAY ATTENTION HERE: we ask the LLM to generate a joke about a subject\n",
    "def generate_joke(state: JokeState):\n",
    "    prompt = joke_prompt.format(subject=state[\"subject\"])\n",
    "    response = model.with_structured_output(Joke).invoke(prompt)\n",
    "    return {\"jokes\": [response.joke]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9b75a-47f5-421e-b590-ebddfae98f42",
   "metadata": {},
   "source": [
    "## Expliquemos el código anterior en términos sencillos\n",
    "\n",
    "El código anterior define cómo generar un chiste sobre un subtema específico utilizando un LLM. Desglosemos los pasos:\n",
    "\n",
    "#### Definición de estructuras de datos\n",
    "- `JokeState` y `Joke` funcionan como **planos** para organizar los datos.\n",
    "\n",
    "  - `JokeState` indica que necesitamos un diccionario (o un objeto) con una key llamada `subject` que contiene un string de texto. Por ejemplo:\n",
    "    ```python\n",
    "    state = {\"subject\": \"gatos\"}\n",
    "    ```\n",
    "\n",
    "  - `Joke` define el formato de output esperado: contiene una única key, `joke`, que también es un string de texto. Por ejemplo:\n",
    "    ```python\n",
    "    {\"joke\": \"¿Por qué el gato se sentó en el ordenador? Para vigilar al ratón.\"}\n",
    "    ```\n",
    "\n",
    "#### La función `generate_joke`\n",
    "Esta es la función principal que crea un chiste basado en el subtema dado. Veamos qué sucede dentro:\n",
    "\n",
    "1. **Obtener el subtema**:\n",
    "   - El parámetro `state` se espera que contenga el subtema del chiste (como `\"gatos\"`).\n",
    "   \n",
    "2. **Preparar un prompt**:\n",
    "   - Se crea la variable `prompt` usando una plantilla predefinida (`joke_prompt`), donde se inserta el subtema. Por ejemplo, si `state[\"subject\"]` es `\"gatos\"`, el prompt se vería así:\n",
    "     ```python\n",
    "     \"Cuéntame un chiste gracioso sobre gatos.\"\n",
    "     ```\n",
    "\n",
    "3. **Llamar al modelo de lenguaje (LLM)**:\n",
    "   - `model.with_structured_output(Joke).invoke(prompt)` envía el prompt al LLM.\n",
    "   - El LLM genera una respuesta que sigue la estructura de `Joke`, asegurando que devuelve un diccionario con una key `joke`.\n",
    "\n",
    "4. **Devolver el chiste**:\n",
    "   - La respuesta del LLM se envuelve en otro diccionario bajo la clave `\"jokes\"`, por lo que el output final podría ser:\n",
    "     ```python\n",
    "     {\"jokes\": [\"¿Por qué el gato se sentó en el ordenador? Para vigilar al ratón.\"]}\n",
    "     ```\n",
    "\n",
    "#### En resumen\n",
    "- La función recibe un subtema (por ejemplo, `\"gatos\"`).\n",
    "- Crea un prompt para pedirle al LLM que genere un chiste sobre ese subtema.\n",
    "- El LLM responde con un chiste.\n",
    "- La función devuelve el chiste en un formato estructurado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26be68-b6dc-482f-8c2f-6674d7dd819d",
   "metadata": {},
   "source": [
    "## Y, finalmente, definamos la función para seleccionar el mejor chiste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "656d3f71-c579-42f4-9cc5-47ec22739f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAY ATTENTION HERE: we ask the LLM to select the best joke\n",
    "def best_joke(state: OverallState):\n",
    "    jokes = \"\\n\\n\".join(state[\"jokes\"])\n",
    "    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "    response = model.with_structured_output(BestJoke).invoke(prompt)\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][response.id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b30e11-9f09-4e0d-ab10-1167f6413480",
   "metadata": {},
   "source": [
    "## OK. Revisemos lo que acabamos de hacer\n",
    "\n",
    "El código anterior define una función que ayuda a elegir el mejor chiste de una lista utilizando un modelo de lenguaje (LLM). Veamos paso a paso cómo funciona:\n",
    "\n",
    "#### Datos del input (`state`)\n",
    "- La función trabaja con un diccionario llamado `state` (que sigue la estructura `OverallState`). Contiene dos piezas clave de información:\n",
    "  1. **`state[\"jokes\"]`**: Una lista de chistes (por ejemplo, `[\"¿Por qué cruzó la gallina la carretera? ¡Para llegar al otro lado!\", \"¿Cómo se llama la pasta falsa? ¡Un impasta!\"]`).\n",
    "  2. **`state[\"topic\"]`**: El tema con el que están relacionados los chistes (por ejemplo, `\"animales\"`).\n",
    "\n",
    "#### Unir los chistes\n",
    "- Los chistes de `state[\"jokes\"]` se combinan en un solo string de texto, separados por dos saltos de línea (`\\n\\n`). Por ejemplo:\n",
    "  ```python\n",
    "  jokes = \"¿Por qué cruzó la gallina la carretera? ¡Para llegar al otro lado!\\n\\n¿Cómo se llama la pasta falsa? ¡Un impasta!\"\n",
    "  ```\n",
    "\n",
    "#### Creación del prompt\n",
    "- La función construye un prompt usando una plantilla (`best_joke_prompt`) para pedirle al LLM que seleccione el mejor chiste. Este incluye:\n",
    "  - El **tema** de los chistes.\n",
    "  - La **lista de chistes**.\n",
    "\n",
    "El prompt sería algo como:\n",
    "  ```text\n",
    "  \"Aquí tienes algunos chistes sobre animales:\n",
    "  \n",
    "  1. ¿Por qué cruzó la gallina la carretera? ¡Para llegar al otro lado!\n",
    "  \n",
    "  2. ¿Cómo se llama la pasta falsa? ¡Un impasta!\n",
    "  \n",
    "  ¿Cuál es el mejor chiste? Responde con el número del chiste.\"\n",
    "  ```\n",
    "\n",
    "#### Consultando al LLM\n",
    "- La línea `model.with_structured_output(BestJoke).invoke(prompt)` envía el prompt al LLM y le pide que seleccione el mejor chiste.\n",
    "- La estructura `BestJoke` garantiza que la respuesta incluya un `id`, que identifica el número o la posición del chiste seleccionado.\n",
    "\n",
    "#### Devolviendo el mejor chiste\n",
    "- La función toma el `id` de la respuesta del LLM y lo usa para buscar el chiste correspondiente en la lista `state[\"jokes\"]`.\n",
    "- El mejor chiste se devuelve en un diccionario bajo la clave `\"best_selected_joke\"`. Por ejemplo:\n",
    "  ```python\n",
    "  {\"best_selected_joke\": \"¿Por qué cruzó la gallina la carretera? ¡Para llegar al otro lado!\"}\n",
    "  ```\n",
    "\n",
    "#### En resumen\n",
    "- La función toma una lista de chistes y un tema.\n",
    "- Pide al LLM que seleccione el mejor chiste mediante un prompt claro.\n",
    "- Devuelve el chiste seleccionado en un formato fácil de usar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14772711-8682-47f7-b622-4b95871cb097",
   "metadata": {},
   "source": [
    "## OK. Ahora pongamos todo junto en nuestra aplicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c58107-485e-46f7-8484-8b984f2b38f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# Construct the graph: here we put everything together to construct our graph\n",
    "graph = StateGraph(OverallState)\n",
    "\n",
    "# Node to generate subjects related to the topic\n",
    "graph.add_node(\"generate_subjects\", generate_subjects)\n",
    "\n",
    "# Node to generate a joke about a subject\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "\n",
    "# Node to select the best joke\n",
    "graph.add_node(\"best_joke\", best_joke)\n",
    "\n",
    "graph.add_edge(START, \"generate_subjects\")\n",
    "\n",
    "# PAY ATTENTION HERE: see how we use the continue_to_jokes function we created with Send\n",
    "graph.add_conditional_edges(\"generate_subjects\", continue_to_jokes, [\"generate_joke\"])\n",
    "\n",
    "graph.add_edge(\"generate_joke\", \"best_joke\")\n",
    "graph.add_edge(\"best_joke\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318cb74f-c98a-4878-9b81-121625965f5d",
   "metadata": {},
   "source": [
    "## OK. Revisemos lo que sucede bajo el capó en esta aplicación\n",
    "\n",
    "La aplicación define un **workflow de tipo map-reduce** en LangGraph para generar y evaluar chistes basados en un tema dado. Aquí tienes una explicación simplificada, paso a paso:\n",
    "\n",
    "#### Propósito del código\n",
    "El código construye un **flujo de trabajo basado en un graph** para:\n",
    "1. Generar una lista de subtemas relacionados con un tema principal (**fase de mapeo**).  \n",
    "2. Crear un chiste para cada subtema (**fase de mapeo**).  \n",
    "3. Seleccionar el mejor chiste de los generados (**fase de reducción**).  \n",
    "\n",
    "#### Componentes clave\n",
    "\n",
    "1. **Entradas (inputs) y salidas (outputs)**:\n",
    "   - **Entrada**: Un tema proporcionado por el usuario (por ejemplo, \"Tecnología\").  \n",
    "   - **Salida**: El mejor chiste relacionado con ese tema.\n",
    "\n",
    "2. **Configuración del modelo**:\n",
    "   - Utiliza el LLM seleccionado (por ejemplo, ChatGPT-4) para generar respuestas basadas en prompts.\n",
    "\n",
    "3. **Modelos de datos**:\n",
    "   - **Subjects**: Contiene la lista de subtemas generados.  \n",
    "   - **Joke**: Almacena un chiste para un subtema específico.  \n",
    "   - **BestJoke**: Guarda el ID del mejor chiste seleccionado.\n",
    "\n",
    "4. **Gestión del estado**:\n",
    "   - **OverallState**: Rastrea el tema, los subtemas generados, los chistes y el mejor chiste seleccionado.  \n",
    "   - **JokeState**: Rastrea el subtema específico mientras se genera un chiste.\n",
    "\n",
    "#### Desglose del flujo de trabajo\n",
    "\n",
    "1. **Generación de subtemas** (Fase de Mapeo):\n",
    "   - **Función**: `generate_subjects()`\n",
    "   - Genera 3 subtemas basados en el tema principal usando el modelo de IA.\n",
    "\n",
    "2. **Generación de chistes** (Fase de Mapeo):\n",
    "   - **Función**: `generate_joke()`\n",
    "   - Crea un chiste para cada subtema.  \n",
    "   - Se ejecuta en paralelo, lo que significa que todos los chistes se generan simultáneamente.\n",
    "\n",
    "3. **Mapeo dinámico**:\n",
    "   - **Función**: `continue_to_jokes()`\n",
    "   - Mapea dinámicamente cada subtema generado al nodo de creación de chistes, incluso si el número de subtemas es desconocido de antemano.\n",
    "\n",
    "4. **Selección del mejor chiste** (Fase de Reducción):\n",
    "   - **Función**: `best_joke()`\n",
    "   - Combina todos los chistes generados y evalúa cuál es el mejor.  \n",
    "\n",
    "#### Construcción del graph\n",
    "\n",
    "- **Nodos**: Representan tareas como generar temas, crear chistes y seleccionar el mejor chiste.  \n",
    "- **Edges**: Definen las transiciones entre tareas.  \n",
    "- **Mapeo dinámico**: Permite manejar inputs de longitud variable (por ejemplo, múltiples subtemas) sin necesidad de predefinir su cantidad.  \n",
    "\n",
    "#### Conceptos clave en acción\n",
    "\n",
    "- **Fase de Mapeo**:  \n",
    "  - Divide el tema en subtemas y procesa cada subtema en paralelo para generar chistes.\n",
    "\n",
    "- **Fase de Reducción**:  \n",
    "  - Combina los resultados (chistes) y selecciona el mejor.\n",
    "\n",
    "- **Flexibilidad**:  \n",
    "  - Maneja números desconocidos de subtemas dinámicamente mediante edges condicionales.\n",
    "\n",
    "#### Analogía con el mundo real\n",
    "\n",
    "Imagina que quieres preparar un show de comedia basado en un tema:  \n",
    "1. **Tema**: \"Tecnología\".  \n",
    "2. **Paso 1 (Mapeo)**: Haces una lluvia de ideas sobre temas relacionados como \"IA\", \"Smartphones\" y \"Robots\".  \n",
    "3. **Paso 2 (Mapeo)**: Escribes chistes sobre cada uno de esos temas.  \n",
    "4. **Paso 3 (Reducción)**: Seleccionas el chiste más gracioso para contar en el show.\n",
    "\n",
    "#### Resultado final\n",
    "\n",
    "El graph ejecuta estos pasos, procesa cada etapa y devuelve el **mejor chiste** relacionado con el tema dado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6256e67f-20a5-4707-8d82-ffa60a820a4a",
   "metadata": {},
   "source": [
    "## Prestemos atención a la línea donde activamos la \"Magia de Send\" en el código anterior\n",
    "\n",
    "En el código anterior, es importante comprender la línea donde activamos la **\"Magia de Send\"**:\n",
    "\n",
    "```python\n",
    "graph.add_conditional_edges(\"generate_subjects\", continue_to_jokes, [\"generate_joke\"])\n",
    "```\n",
    "\n",
    "#### Explicación\n",
    "Aquí está el desglose de sus componentes:\n",
    "\n",
    "1. **`graph.add_conditional_edges`**:\n",
    "   - Esta función define **edges condicionales** en el graph de state que se crean dinámicamente según alguna condición o cálculo. A diferencia de una edge estático (como `graph.add_edge`), los edges condicionales dependen del resultado de una función definida por el usuario, la cual se evalúa en tiempo de ejecución.\n",
    "\n",
    "2. **`\"generate_subjects\"`**:\n",
    "   - Este es el nodo de origen en el graph. Después de que este nodo se procesa, el graph evalúa la función `continue_to_jokes` para determinar qué sucede a continuación.\n",
    "\n",
    "3. **`continue_to_jokes`**:\n",
    "   - Esta es la función personalizada que determina los edges condicionales.  \n",
    "   - Toma el state actual como entrada (`OverallState`) y devuelve una lista de instrucciones `Send`, que indican a qué nodo ir a continuación y qué datos deben pasarse.  \n",
    "   - En este caso, por cada subtema en `state[\"subjects\"]`, se genera una instrucción `Send` que indica que el flujo debe pasar al nodo `\"generate_joke\"` y que debe enviarse el subtema correspondiente en los datos.\n",
    "\n",
    "   Aquí está la función `continue_to_jokes`:\n",
    "   ```python\n",
    "   def continue_to_jokes(state: OverallState):\n",
    "       return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n",
    "   ```\n",
    "   - **`Send(\"generate_joke\", {\"subject\": s})`**:\n",
    "     - Esto indica al graph que envíe el flujo de control al nodo `\"generate_joke\"`, pasando un diccionario `{\"subject\": s}` como datos de entrada, donde `s` es cada subtema dentro de la lista `state[\"subjects\"]`.\n",
    "\n",
    "4. **`[\"generate_joke\"]`**:\n",
    "   - **Esta es la lista de nodos de destino posibles**. En este caso, hay un solo nodo de destino: `\"generate_joke\"`, donde la función `generate_joke` procesará los datos enviados por cada `Send`.\n",
    "\n",
    "#### ¿Qué sucede en la práctica?\n",
    "- Después de que el nodo `\"generate_subjects\"` se ejecuta, llena el estado con una lista de subtemas (`state[\"subjects\"]`).\n",
    "- Se invoca la función `continue_to_jokes`, que crea una instrucción `Send` por cada subtema en la lista.\n",
    "- Cada instrucción `Send` crea dinámicamente una conexión al nodo `\"generate_joke\"`, enviando el subtema correspondiente como input.\n",
    "- **El graph procesa el nodo `\"generate_joke\"` múltiples veces —una vez por cada subtema— generando un chiste para cada uno**.\n",
    "\n",
    "#### Analogía visual\n",
    "Imagina que `\"generate_subjects\"` es una máquina que genera una lista de subtemas como `[\"gatos\", \"perros\", \"pingüinos\"]`.  \n",
    "La función `continue_to_jokes` actúa como un distribuidor, enviando cada subtema al nodo `\"generate_joke\"` por separado. Por ejemplo:\n",
    "- `\"gatos\"` → `\"generate_joke\"`\n",
    "- `\"perros\"` → `\"generate_joke\"`\n",
    "- `\"pingüinos\"` → `\"generate_joke\"`\n",
    "\n",
    "Esto asegura que cada subtema se procese de forma independiente.\n",
    "\n",
    "#### Resumen\n",
    "Esta línea de código:\n",
    "```python\n",
    "graph.add_conditional_edges(\"generate_subjects\", continue_to_jokes, [\"generate_joke\"])\n",
    "```\n",
    "permite que el nodo `\"generate_subjects\"` se conecte dinámicamente al nodo `\"generate_joke\"` para **cada subtema** en la lista `state[\"subjects\"]`, permitiendo que se generen chistes de manera independiente para cada uno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357b5a1-9849-4a3c-a9d7-3837aec0abaf",
   "metadata": {},
   "source": [
    "## Y ahora podemos ejecutar la aplicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad06095-df25-4fc5-b9f6-7126f9b8bd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_subjects': {'subjects': ['Mammals', 'Reptiles', 'Birds']}}\n",
      "{'generate_joke': {'jokes': [\"Why don't mammals ever use computers? Because they're afraid of the mouse!\"]}}\n",
      "{'generate_joke': {'jokes': [\"Why don't reptiles like fast food?\\nBecause they can't catch it!\"]}}\n",
      "{'generate_joke': {'jokes': [\"Why do seagulls fly over the ocean? Because if they flew over the bay, they'd be called bagels!\"]}}\n",
      "{'best_joke': {'best_selected_joke': \"Why do seagulls fly over the ocean? Because if they flew over the bay, they'd be called bagels!\"}}\n"
     ]
    }
   ],
   "source": [
    "# Call the graph: here we call it to generate a list of jokes\n",
    "for s in app.stream({\"topic\": \"animals\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf77e71-99cc-4fab-a7f3-2084d647b025",
   "metadata": {},
   "source": [
    "## Cómo ejecutar el código desde Visual Studio Code\n",
    "\n",
    "* En Visual Studio Code, localiza el archivo **`022-map-reduce.py`**.  \n",
    "* En la terminal, asegúrate de que estás en el directorio donde se encuentra el archivo y ejecuta:  \n",
    "    ```bash\n",
    "    python 022-map-reduce.py\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea29c8-7b7d-4db8-b597-827f426fc333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
