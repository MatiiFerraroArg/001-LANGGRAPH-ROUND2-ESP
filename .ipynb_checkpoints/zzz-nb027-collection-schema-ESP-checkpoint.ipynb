{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da057a27-6921-44b0-af22-c6b4fb17ecc4",
   "metadata": {},
   "source": [
    "# Usando schemas de memoria (Memory Schemas) complejos para operaciones avanzadas (2): Guardando memories en collections.\n",
    "* En el ejercicio anterior aprendimos a guardar recuerdos (memories) en un solo perfil de usuario, pero ¿qué pasa si queremos guardar recuerdos en una colección (collection) en lugar de un solo perfil? Aprenderemos cómo hacerlo en este ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d579ef3-83fd-4d92-a5a4-95c2cb18e57e",
   "metadata": {},
   "source": [
    "## ¿Qué es una colección (collection) y por qué querríamos usarla?\n",
    "\n",
    "Aclaremos qué significa esto paso a paso:\n",
    "\n",
    "#### Perfil único de usuario\n",
    "En el ejercicio anterior, guardamos todos los **recuerdos** (memories, fragmentos de información) en un **único perfil** para un usuario.\n",
    "\n",
    "Por ejemplo:\n",
    "- El Usuario 1 tiene un perfil con este recuerdo (memories):\n",
    "  ```\n",
    "  \"El usuario disfruta aprender sobre francés.\"\n",
    "  ```\n",
    "- Todos los recuerdos se almacenan **en un solo lugar**, solo para **un usuario**.\n",
    "\n",
    "#### Colección (collection)\n",
    "Ahora, este ejercicio introduce la idea de guardar **recuerdos en una colección** en lugar de vincularlos a un **perfil único**.\n",
    "\n",
    "Una **colección (collection)** es como una **carpeta** o una **base de datos** que puede almacenar **recuerdos de múltiples usuarios**.\n",
    "\n",
    "Ejemplo:\n",
    "- Colección de recuerdos para **Usuario 1**:\n",
    "  ```\n",
    "  [\"Al usuario le gusta aprender sobre el idioma francés.\", \"El usuario prefiere cursos online.\"]\n",
    "  ```\n",
    "\n",
    "- Colección de recuerdos para **Usuario 2**:  \n",
    "  ```\n",
    "  [\"El usuario quiere aprender español.\", \"El usuario prefiere clases presenciales.\"]\n",
    "  ```\n",
    "\n",
    "#### Diferencia clave\n",
    "- **Perfil único:** Almacena recuerdos para **un solo usuario**.  \n",
    "- **Colección (collection):** Almacena **recuerdos de múltiples usuarios** o **múltiples temas** de manera organizada.\n",
    "\n",
    "#### ¿Por qué usar una colección (collection)?\n",
    "Una colección facilita:\n",
    "1. Almacenar recuerdos de **múltiples usuarios**.  \n",
    "2. **Recuperar recuerdos específicos** para un usuario determinado.  \n",
    "3. **Escalar el sistema** a medida que se agregan más usuarios o recuerdos.  \n",
    "\n",
    "#### Analogía\n",
    "- **Perfil único** = Un cuaderno para **un estudiante**.  \n",
    "- **Colección** = Una **estantería** con cuadernos separados para **muchos estudiantes**.\n",
    "\n",
    "Este enfoque es útil cuando tu aplicación o sistema necesita manejar **más de un usuario** o **datos más complejos**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e336-d054-412c-8a3f-1fbec63e1bcd",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda8d4-80cf-4b8f-9981-94edda5e9911",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 027-collection-schema.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-langgraph**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e766aa-f3e2-491f-be99-d0c6b700d47a",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df978ec5-bfd2-4167-bd33-86bc2687d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel35 = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chatModel4o = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb7ebe-726c-4fa9-9854-15d0fbde3a19",
   "metadata": {},
   "source": [
    "## Nuestro objetivo: almacenar los recuerdos sobre las interacciones del usuario en una colección (collection)  \n",
    "* En lugar de almacenar la información del usuario en una estructura de perfil fija, queremos crear un **schema de collection flexible** para almacenar recuerdos sobre las interacciones del usuario.  \n",
    "    * Cada recuerdo se almacenará como una entrada separada con un único campo `content` que contendrá la información principal que queremos recordar.  \n",
    "* Este enfoque nos permite construir una colección de recuerdos abierta, que **puede crecer y cambiar a medida que aprendemos más sobre el usuario**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400d299-3489-4aa8-8657-439754425233",
   "metadata": {},
   "source": [
    "## Comencemos definiendo el schema de la collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76db7850-d968-4352-8cb6-c9f8615d5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: list[Memory] = Field(description=\"A list of memories about the user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a040c8-4851-4140-a9c3-d779abcd3972",
   "metadata": {},
   "source": [
    "## OK. Revisemos lo que acabamos de hacer.  \n",
    "\n",
    "El código anterior define dos clases (class) utilizando **Pydantic**, una biblioteca de Python para validación de datos y gestión de configuraciones. Aquí tienes una explicación sencilla:  \n",
    "\n",
    "#### Clase `Memory`\n",
    "- Representa un **fragmento de información** (recuerdo) sobre el usuario.  \n",
    "- Contiene **un atributo**:  \n",
    "  - **`content`** (un string de texto) – Almacena el recuerdo en sí, como \"El usuario quiere aprender francés.\"  \n",
    "  - Incluye una **descripción** que explica el propósito de este campo.  \n",
    "\n",
    "#### Clase `MemoryCollection`\n",
    "- Representa una **colección de recuerdos** sobre el usuario.  \n",
    "- Contiene **un atributo**:  \n",
    "  - **`memories`** – Una **list de objetos `Memory`** para almacenar múltiples recuerdos del usuario.  \n",
    "  - También tiene una **descripción** que explica su propósito.  \n",
    "\n",
    "#### ¿Por qué usar Pydantic?\n",
    "- **Validación:** Garantiza que los datos sigan el formato esperado (por ejemplo, `content` debe ser un string de texto).  \n",
    "- **Documentación:** Genera automáticamente documentación clara para el modelo de datos.  \n",
    "\n",
    "#### Ejemplo de uso  \n",
    "\n",
    "```python\n",
    "# Crear un recuerdo\n",
    "memory1 = Memory(content=\"El usuario expresó interés en aprender francés.\")\n",
    "\n",
    "# Crear otro recuerdo\n",
    "memory2 = Memory(content=\"El usuario prefiere cursos en línea.\")\n",
    "\n",
    "# Reunir recuerdos en una lista\n",
    "memory_collection = MemoryCollection(memories=[memory1, memory2])\n",
    "\n",
    "# Acceder a los datos\n",
    "print(memory_collection.memories[0].content)  # Salida: El usuario expresó interés en aprender francés.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9eabfc-2a86-4bb3-adc2-1e288bebc693",
   "metadata": {},
   "source": [
    "## Ahora que hemos definido el schema de la colección, probémoslo con un chatbot sencillo  \n",
    "* Usaremos un chatbot con output estructurado (structured output), similar al que utilizamos en el ejercicio anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0554340-54d5-463c-b3a4-aef9ed3a3f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Memory(content=\"User's name is Julio.\"),\n",
       " Memory(content='Julio likes to drive his vespa in San Francisco.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(MemoryCollection)\n",
    "\n",
    "# Invoke the model to produce structured output that matches the schema\n",
    "memory_collection = model_with_structure.invoke([HumanMessage(\"My name is Julio. I like to drive my vespa in SF.\")])\n",
    "memory_collection.memories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001fa57-ae70-42ff-85fc-617d696b66e4",
   "metadata": {},
   "source": [
    "## OK. Ahora guardemos el recuerdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d8b34f-08c6-435b-be31-548434e83c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize the in-memory store\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[0].model_dump()\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[1].model_dump()\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58251895-7bce-493f-8c36-8c214c052737",
   "metadata": {},
   "source": [
    "## Finalmente, confirmemos que podemos recuperarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5135b907-ac87-4e33-806a-5af7f1cb0f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'content': \"User's name is Julio.\"}, 'key': '02ae72a3-7e7f-4020-9a74-3ebc8008590e', 'namespace': ['1', 'memories'], 'created_at': '2025-01-07T16:18:25.875274+00:00', 'updated_at': '2025-01-07T16:18:25.875280+00:00', 'score': None}\n",
      "{'value': {'content': 'Julio likes to drive his vespa in San Francisco.'}, 'key': '917a1e65-d413-404f-a74f-432c0c55a6d4', 'namespace': ['1', 'memories'], 'created_at': '2025-01-07T16:18:25.875489+00:00', 'updated_at': '2025-01-07T16:18:25.875490+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Search \n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e8b4b-8d42-41ff-a9ab-2677a1a8f3b6",
   "metadata": {},
   "source": [
    "## Bien. Revisemos lo que acabamos de hacer.  \n",
    "\n",
    "El código anterior amplía el ejemplo previo y demuestra cómo **generar datos estructurados**, **almacenarlos en memoria** y **recuperarlos después**. Aquí tienes una explicación sencilla:  \n",
    "\n",
    "#### Importar librerías y configurar el modelo  \n",
    "```python\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "```\n",
    "- Importa herramientas para trabajar con mensajes de chat y modelos GPT de OpenAI.  \n",
    "\n",
    "```python\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "```\n",
    "- Inicializa el modelo **GPT-4o** con una temperatura de **0** (esto garantiza respuestas lo más deterministas posible).  \n",
    "\n",
    "#### Vincular el esquema al modelo  \n",
    "```python\n",
    "model_with_structure = model.with_structured_output(MemoryCollection)\n",
    "```\n",
    "- Configura el modelo para producir una salida estructurada según el esquema **`MemoryCollection`** creado anteriormente.  \n",
    "- Esto garantiza que las respuestas sean **estructuradas** y fáciles de procesar.  \n",
    "\n",
    "#### Generar salida estructurada  \n",
    "```python\n",
    "memory_collection = model_with_structure.invoke(\n",
    "    [HumanMessage(\"My name is Julio. I like to drive my vespa in SF.\")]\n",
    ")\n",
    "memory_collection.memories\n",
    "```\n",
    "- Envía un **mensaje de usuario** al modelo y le pide generar una respuesta estructurada.  \n",
    "- El output se almacena como un objeto **`MemoryCollection`**, siguiendo el schema definido antes.  \n",
    "- Por ejemplo:  \n",
    "  - Recuerdo 1: `\"My name is Julio.\"`  \n",
    "  - Recuerdo 2: `\"I like to drive my vespa in SF.\"`  \n",
    "\n",
    "#### Importar y configurar almacenamiento en memoria  \n",
    "```python\n",
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "```\n",
    "- Importa herramientas para **almacenamiento en memoria** (ideal para acceso rápido).  \n",
    "- Crea una instancia de almacenamiento (`in_memory_store`) para guardar y gestionar los recuerdos.  \n",
    "\n",
    "#### Guardar recuerdos en el almacenamiento  \n",
    "```python\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "```\n",
    "- Define un **namespace** (como una carpeta) usando un **ID de usuario** y la etiqueta `\"memories\"`.  \n",
    "- Esto organiza los recuerdos por usuario y tipo.  \n",
    "\n",
    "```python\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[0].model_dump()\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "```\n",
    "- Genera una **clave única (unique key)** para cada recuerdo usando `uuid`.  \n",
    "- Convierte cada recuerdo en **formato diccionario** para almacenarlo.  \n",
    "- Guarda el recuerdo en el namespace con su clave y valor (key and value).  \n",
    "\n",
    "#### Buscar y recuperar recuerdos  \n",
    "```python\n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())\n",
    "```\n",
    "- Busca todos los recuerdos almacenados en el namespace.  \n",
    "- Los imprime en formato diccionario, facilitando su lectura o procesamiento posterior.  \n",
    "\n",
    "#### Ejemplo de output  \n",
    "```\n",
    "{'content': 'My name is Julio.'}\n",
    "{'content': 'I like to drive my vespa in SF.'}\n",
    "```\n",
    "\n",
    "Este proceso permite estructurar, almacenar y recuperar información de manera eficiente, lo que es útil para construir sistemas con memoria persistente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0504dea-3df4-4c82-a82f-ff3aaa42cc3d",
   "metadata": {},
   "source": [
    "## Agregar y editar recuerdos en la collection usando TrustCall  \n",
    "* Recuerda que en el último ejercicio aprendimos a usar **TrustCall**.  \n",
    "* Lo utilizaremos nuevamente aquí para **agregar y editar recuerdos** en nuestra collection.  \n",
    "* Como verás a continuación, configuraremos **`enable_inserts=True`** para permitir que el extractor de TrustCall **inserte nuevos recuerdos** en la collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "344bedd2-dcd3-46c0-84dc-dedf60df9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustcall import create_extractor\n",
    "\n",
    "# Create the extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0987441b-7f74-4758-9469-7da0bc08d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Instruction\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "\n",
    "# Conversation\n",
    "conversation = [HumanMessage(content=\"Hi, I'm Julio.\"), \n",
    "                AIMessage(content=\"Nice to meet you, Julio.\"), \n",
    "                HumanMessage(content=\"This morning I had a nice vespa ride in San Francisco.\")]\n",
    "\n",
    "# Invoke the extractor\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=instruction)] + conversation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69174615-5604-4c52-b271-d04fd7ac6daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_Zq95MSs0HxLY2V3CZoLRDswM)\n",
      " Call ID: call_Zq95MSs0HxLY2V3CZoLRDswM\n",
      "  Args:\n",
      "    content: Julio had a nice vespa ride in San Francisco this morning.\n"
     ]
    }
   ],
   "source": [
    "# Messages contain the tool calls\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c66ac5-d507-4965-864e-d25144b13385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Julio had a nice vespa ride in San Francisco this morning.'\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02be1a64-a251-42e8-8891-59160dcbd022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_Zq95MSs0HxLY2V3CZoLRDswM'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72a5e59a-478a-4dd8-b92a-b84d6b66679e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0',\n",
       "  'Memory',\n",
       "  {'content': 'Julio had a nice vespa ride in San Francisco this morning.'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the conversation\n",
    "updated_conversation = [AIMessage(content=\"That's great, what did you do after?\"), \n",
    "                        HumanMessage(content=\"I went to Whole Foods and bought a green pie soup.\"),                        \n",
    "                        AIMessage(content=\"What else is on your mind?\"),\n",
    "                        HumanMessage(content=\"I was thinking about my trip to Carmel, and going back this weekend.\"),]\n",
    "\n",
    "# Update the instruction\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
    "\n",
    "# We'll save existing memories, giving them an ID, key (tool name), and value\n",
    "tool_name = \"Memory\"\n",
    "existing_memories = [(str(i), tool_name, memory.model_dump()) for i, memory in enumerate(result[\"responses\"])] if result[\"responses\"] else None\n",
    "existing_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af5766c9-89bf-4341-99df-ed3c505f5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the extractor with our updated conversation and existing memories\n",
    "result = trustcall_extractor.invoke({\"messages\": updated_conversation, \n",
    "                                     \"existing\": existing_memories})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6e7cd37-3a60-44e7-9852-c1eaee95cae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_G5ZsqCzkpJLnocBmTKvp4LlK)\n",
      " Call ID: call_G5ZsqCzkpJLnocBmTKvp4LlK\n",
      "  Args:\n",
      "    content: Julio had a nice vespa ride in San Francisco this morning. Then, he went to Whole Foods and bought a green pie soup.\n",
      "  Memory (call_aZ0NAlikwEv4USQVxk1f7Ytj)\n",
      " Call ID: call_aZ0NAlikwEv4USQVxk1f7Ytj\n",
      "  Args:\n",
      "    content: Julio was thinking about his trip to Carmel and considering going back this weekend.\n"
     ]
    }
   ],
   "source": [
    "# Messages from the model indicate two tool calls were made\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13b54f85-85d1-4f12-b6bd-5d88a566ff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Julio had a nice vespa ride in San Francisco this morning. Then, he went to Whole Foods and bought a green pie soup.'\n",
      "content='Julio was thinking about his trip to Carmel and considering going back this weekend.'\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b073903a-9547-4e98-80d5-73118e848095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_G5ZsqCzkpJLnocBmTKvp4LlK', 'json_doc_id': '0'}\n",
      "{'id': 'call_aZ0NAlikwEv4USQVxk1f7Ytj'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0c6e7-3f43-4bb0-a617-1bd556f35198",
   "metadata": {},
   "source": [
    "## Bien. Revisemos lo que acabamos de hacer.  \n",
    "\n",
    "El código anterior demuestra cómo usar **TrustCall** para **extraer, editar y agregar recuerdos** a partir de una conversación en un formato estructurado. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Importar TrustCall y crear un extractor  \n",
    "```python\n",
    "from trustcall import create_extractor\n",
    "```\n",
    "- Importa la **biblioteca TrustCall**, que ayuda a extraer información estructurada de texto.  \n",
    "\n",
    "```python\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True,\n",
    ")\n",
    "```\n",
    "- **Crea un extractor** conectado al modelo GPT.  \n",
    "- **`tools=[Memory]`**: Indica que el output debe seguir el schema **`Memory`** definido antes.  \n",
    "- **`tool_choice=\"Memory\"`**: Garantiza que el extractor siempre utilice la herramienta **Memory** para generar salidas estructuradas (structured outputs).  \n",
    "- **`enable_inserts=True`**: Permite **agregar nuevos recuerdos** a la colección (collection).  \n",
    "\n",
    "#### Proporcionar instrucciones y conversación inicial  \n",
    "```python\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "```\n",
    "- Define las **instrucciones** para que el extractor **identifique recuerdos relevantes** en la conversación.  \n",
    "\n",
    "```python\n",
    "conversation = [HumanMessage(content=\"Hi, I'm Julio.\"), \n",
    "                AIMessage(content=\"Nice to meet you, Julio.\"), \n",
    "                HumanMessage(content=\"This morning I had a nice vespa ride in San Francisco.\")]\n",
    "```\n",
    "- Representa el **historial de conversación** con una lista de mensajes.  \n",
    "- **`HumanMessage`** y **`AIMessage`** distinguen entre mensajes del usuario y del modelo de IA (el modelo LLM).  \n",
    "\n",
    "#### Extraer recuerdos de la conversación  \n",
    "```python\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=instruction)] + conversation})\n",
    "```\n",
    "- Combina la **instrucción** y la **conversación** en una **única entrada (input)** para el extractor.  \n",
    "- El extractor **analiza los mensajes** y genera recuerdos estructurados.  \n",
    "\n",
    "```python\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "```\n",
    "- Muestra **cómo el extractor interpretó la entrada (input)**.  \n",
    "\n",
    "```python\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)\n",
    "```\n",
    "- Muestra los **recuerdos extraídos**, por ejemplo:  \n",
    "  - **Recuerdo 1:** `\"Hi, I'm Julio.\"`  \n",
    "  - **Recuerdo 2:** `\"I had a nice vespa ride in San Francisco.\"`  \n",
    "\n",
    "```python\n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)\n",
    "```\n",
    "- Imprime **metadatos**, como qué herramienta se usó y cómo se procesaron los recuerdos.  \n",
    "\n",
    "#### Actualizar la conversación y los recuerdos  \n",
    "\n",
    "**Nueva conversación:**  \n",
    "```python\n",
    "updated_conversation = [AIMessage(content=\"That's great, what did you do after?\"), \n",
    "                        HumanMessage(content=\"I went to Whole Foods and bought a green pie soup.\"),                        \n",
    "                        AIMessage(content=\"What else is on your mind?\"),\n",
    "                        HumanMessage(content=\"I was thinking about my trip to Carmel, and going back this weekend.\")]\n",
    "```\n",
    "- Agrega **nuevo contexto** para extraer o actualizar recuerdos.  \n",
    "\n",
    "**Actualizar la instrucción:**\n",
    "```python\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
    "```\n",
    "- Cambia la instrucción para que el extractor **modifique recuerdos existentes o agregue nuevos**.  \n",
    "\n",
    "**Guardar recuerdos existentes:**\n",
    "```python\n",
    "tool_name = \"Memory\"\n",
    "existing_memories = [(str(i), tool_name, memory.model_dump()) for i, memory in enumerate(result[\"responses\"])] if result[\"responses\"] else None\n",
    "```\n",
    "- Prepara los **recuerdos existentes** para la actualización, convirtiéndolos en una estructura con:  \n",
    "  - **ID** (por ejemplo, `0`, `1`).  \n",
    "  - **Nombre de la herramienta** (`\"Memory\"`).  \n",
    "  - **Valor** (contenido del recuerdo).  \n",
    "\n",
    "#### Actualizar recuerdos con TrustCall  \n",
    "```python\n",
    "result = trustcall_extractor.invoke({\"messages\": updated_conversation, \n",
    "                                     \"existing\": existing_memories})\n",
    "```\n",
    "- Procesa la conversación actualizada junto con los **recuerdos existentes** para:  \n",
    "  1. **Editar recuerdos previos** si es necesario.  \n",
    "  2. **Insertar nuevos recuerdos** según el contexto actualizado.  \n",
    "\n",
    "**Inspeccionar los resultados:**  \n",
    "```python\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "```\n",
    "- Muestra **detalles de los cambios** en los recuerdos.  \n",
    "\n",
    "```python\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)\n",
    "```\n",
    "- Muestra los **nuevos y actualizados recuerdos** en formato estructurado, por ejemplo:  \n",
    "  - **Recuerdo 1:** `\"Julio fue a Whole Foods y compró una sopa de green pies.\"`  \n",
    "  - **Recuerdo 2:** `\"Julio está planeando un viaje a Carmel este fin de semana.\"`  \n",
    "\n",
    "```python\n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)\n",
    "```\n",
    "- Imprime **metadatos** sobre los cambios realizados (inserciones, ediciones).  \n",
    "\n",
    "#### Ejemplo de output con recuerdos actualizados  \n",
    "```\n",
    "{'content': \"Julio fue a Whole Foods y compró una sopa de pastel verde.\"}\n",
    "{'content': \"Julio está planeando un viaje a Carmel este fin de semana.\"}\n",
    "```\n",
    "\n",
    "Este proceso permite **extraer, almacenar y actualizar recuerdos de manera automática**, lo que resulta clave para aplicaciones con memoria persistente y capacidad de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d1ac3-e474-41ec-ac8f-597c5244e800",
   "metadata": {},
   "source": [
    "## Bien, ahora probemos este enfoque con el chatbot que construimos en el último ejercicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37561fc6-a6d5-4068-b394-8bfe7c507c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "import uuid\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.messages import merge_message_runs\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Memory schema\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "# Create the Trustcall extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    # This allows the extractor to insert new memories\n",
    "    enable_inserts=True,\n",
    ")\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot. You are designed to be a companion to a user. \n",
    "\n",
    "You have a long term memory which keeps track of information you learn about the user over time.\n",
    "\n",
    "Current Memory (may include updated memories from this conversation): \n",
    "\n",
    "{memory}\"\"\"\n",
    "\n",
    "# Trustcall instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction. \n",
    "\n",
    "Use the provided tools to retain any necessary memories about the user. \n",
    "\n",
    "Use parallel tool calling to handle updates and insertions simultaneously:\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=info)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n",
    "                          for existing_item in existing_items]\n",
    "                          if existing_items\n",
    "                          else None\n",
    "                        )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]))\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = trustcall_extractor.invoke({\"messages\": updated_messages, \n",
    "                                        \"existing\": existing_memories})\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace,\n",
    "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "                  r.model_dump(mode=\"json\"),\n",
    "            )\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df69b1d4-fa4d-4828-a231-e5cd10aea38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Julio\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Julio! It's great to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2f3e41a-18e6-4b62-bae7-4559b292fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to drive my vespa around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a lot of fun! San Francisco must be a great place to explore on a Vespa with all its unique neighborhoods and scenic views. Do you have any favorite spots you like to visit?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to drive my vespa around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e3d3656-626c-4999-92e9-9697b704aa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'content': \"User's name is Julio.\"}, 'key': '173a1f17-2308-4f16-bb04-9e20ea2f87fc', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:19:09.245031+00:00', 'updated_at': '2025-01-08T09:19:09.245032+00:00', 'score': None}\n",
      "{'value': {'content': 'User likes to drive their Vespa around San Francisco.'}, 'key': '75e9347b-19d7-462b-a662-114209d76729', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:19:34.470361+00:00', 'updated_at': '2025-01-08T09:19:34.470364+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memories\", user_id)\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e451c70-8ef4-4a9a-b2f8-7c627ae92c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I also enjoy going to Whole Foods\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Whole Foods is a great place for fresh and organic groceries. Do you have any favorite items you like to pick up when you're there?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I also enjoy going to Whole Foods\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bf047-dff7-4dbf-b724-60bf7ee18383",
   "metadata": {},
   "source": [
    "#### Pay attention: now we will continue the conversation with a new thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f9d43fa-0df8-4bfe-90f9-7b74508525d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you recommend for me to buy in Whole Foods?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Since you enjoy going to Whole Foods, you might want to explore some of their fresh produce or organic options. If you're looking for something new, you could try their seasonal fruits and vegetables, or perhaps check out their selection of artisanal cheeses and freshly baked bread. If you're into cooking, their spice and herb section is also worth a look. Do you have any specific preferences or dietary needs?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"What do you recommend for me to buy in Whole Foods?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95bd7ced-b972-472d-9077-ecd3af90eb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'content': \"User's name is Julio.\"}, 'key': '173a1f17-2308-4f16-bb04-9e20ea2f87fc', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:19:09.245031+00:00', 'updated_at': '2025-01-08T09:19:09.245032+00:00', 'score': None}\n",
      "{'value': {'content': 'User likes to drive their Vespa around San Francisco.'}, 'key': '75e9347b-19d7-462b-a662-114209d76729', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:19:34.470361+00:00', 'updated_at': '2025-01-08T09:19:34.470364+00:00', 'score': None}\n",
      "{'value': {'content': 'User enjoys going to Whole Foods.'}, 'key': '55ba8966-aee1-4582-aa9a-9fe2250a3ade', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:20:22.873059+00:00', 'updated_at': '2025-01-08T09:20:22.873061+00:00', 'score': None}\n",
      "{'value': {'content': 'User is interested in recommendations for purchases at Whole Foods.'}, 'key': '0e3e60e8-adbb-4818-8af4-c4feadca4592', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:22:44.080135+00:00', 'updated_at': '2025-01-08T09:22:44.080136+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# PAY ATTENTION HERE: see how the second threat\n",
    "# is also saved in the user memory.\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memories\", user_id)\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cffaaff-324f-4996-b8f3-9a9539de742c",
   "metadata": {},
   "source": [
    "## Bien. Revisemos lo que acabamos de hacer\n",
    "\n",
    "El código anterior demuestra cómo crear un **chatbot de IA con memoria a largo plazo** usando **LangGraph** y **TrustCall**. El chatbot puede **extraer, almacenar, actualizar y usar recuerdos** de conversaciones para hacer las interacciones más personalizadas. Aquí tienes una **explicación simplificada**:\n",
    "\n",
    "\n",
    "#### 1. Componentes clave\n",
    "\n",
    "**1.1 Schema de memoria**\n",
    "```python\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"Almacena información del usuario.\")\n",
    "```\n",
    "- Define una **estructura de memoria** para guardar detalles importantes del usuario (por ejemplo, \"A Julio le gusta conducir motos vespa\").  \n",
    "\n",
    "\n",
    "**1.2 Extractor de TrustCall**\n",
    "```python\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True,\n",
    ")\n",
    "```\n",
    "- Usa **TrustCall** para **extraer y actualizar recuerdos** de las conversaciones.  \n",
    "- **`enable_inserts=True`:** Permite agregar **nuevos recuerdos** sin eliminar los antiguos.  \n",
    "\n",
    "\n",
    "**1.3 Instrucciones para el comportamiento de la IA (el modelo LLM)**\n",
    "```python\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"Eres un chatbot útil. Tienes una memoria a largo plazo para los detalles del usuario.\"\"\"\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflexiona sobre la conversación y actualiza o inserta nuevos recuerdos.\"\"\"\n",
    "```\n",
    "- Guía al modelo de IA (el modelo LLM) para:\n",
    "  1. **Usar los recuerdos** en sus respuestas.  \n",
    "  2. **Actualizar o crear recuerdos** después de cada interacción.  \n",
    "\n",
    "#### 2. Funciones de gestión de memoria\n",
    "\n",
    "**2.1 Responder con el contexto de memoria**\n",
    "```python\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=info)\n",
    "\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "```\n",
    "- **Carga los recuerdos almacenados** del usuario.  \n",
    "- **Personaliza las respuestas** incluyendo recuerdos relevantes (por ejemplo, \"A Julio le gusta conducir una moto vespa\").  \n",
    "\n",
    "\n",
    "**2.2 Actualizar recuerdos**\n",
    "```python\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories = ([(item.key, tool_name, item.value) for item in existing_items] if existing_items else None)\n",
    "\n",
    "    updated_messages = list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]))\n",
    "    result = trustcall_extractor.invoke({\"messages\": updated_messages, \"existing\": existing_memories})\n",
    "\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace, rmeta.get(\"json_doc_id\", str(uuid.uuid4())), r.model_dump(mode=\"json\"))\n",
    "```\n",
    "- **Analiza el historial del chat** para extraer recuerdos nuevos o actualizados.  \n",
    "- **Los almacena** en la memoria para su uso futuro.  \n",
    "\n",
    "\n",
    "#### 3. Construir el flujo del chatbot\n",
    "\n",
    "**3.1 Crear el graph para el flujo de conversación**\n",
    "```python\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "```\n",
    "- **Define el flujo** del chatbot:  \n",
    "  1. Responde basándose en la memoria.  \n",
    "  2. Actualiza o inserta nuevos recuerdos después de la respuesta.  \n",
    "\n",
    "\n",
    "**3.2 Configurar el almacenamiento de memoria**\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "```\n",
    "- **Memoria a corto plazo:** Rastrea el historial del chat durante la sesión.  \n",
    "- **Memoria a largo plazo:** Almacena detalles importantes del usuario para futuras sesiones.  \n",
    "\n",
    "\n",
    "#### 4. Interacciones y uso de memoria\n",
    "\n",
    "**4.1 Mensajes de entrada (inputs)**\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"Hola, mi nombre es Julio\")]\n",
    "```\n",
    "- El usuario se presenta, y este detalle se **guarda como un recuerdo**.  \n",
    "\n",
    "**4.2 Procesar y responder**\n",
    "```python\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "- **Envía la entrada (input) del usuario** a través del flujo del chatbot.  \n",
    "- **Almacena el recuerdo:** \"El nombre del usuario es Julio.\"  \n",
    "\n",
    "\n",
    "**4.3 Almacenar y recuperar recuerdos**\n",
    "```python\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())\n",
    "```\n",
    "- **Recupera los recuerdos almacenados** y los imprime:\n",
    "  ```\n",
    "  {'content': 'El nombre del usuario es Julio.'}\n",
    "  ```\n",
    "\n",
    "\n",
    "**4.4 Continuar conversaciones**\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"Me gusta conducir mi vespa por San Francisco\")]\n",
    "```\n",
    "- Agrega más detalles del usuario, almacenados como:\n",
    "  ```\n",
    "  {'content': 'Al usuario le gusta conducir una vespa por San Francisco.'}\n",
    "  ```\n",
    "\n",
    "**4.5 Cambiar de contexto**\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "```\n",
    "- **Cambia el contexto de la conversación** (por ejemplo, diferente tema o sesión) mientras sigue **usando los recuerdos almacenados** sobre Julio.  \n",
    "\n",
    "\n",
    "#### 5. Ejemplo de output\n",
    "\n",
    "**Mensajes del usuario:**\n",
    "1. \"Hola, mi nombre es Julio.\"  \n",
    "2. \"Me gusta conducir mi vespa por San Francisco.\"  \n",
    "3. \"También me gusta ir a Whole Foods.\"  \n",
    "\n",
    "\n",
    "**Recuerdos almacenados:**\n",
    "```\n",
    "{'content': \"El nombre del usuario es Julio.\"}\n",
    "{'content': \"Al usuario le gusta conducir una vespa por San Francisco.\"}\n",
    "{'content': \"Al usuario le gusta ir a Whole Foods.\"}\n",
    "```\n",
    "\n",
    "\n",
    "**Próxima interacción:**\n",
    "Entrada del usuario:  \n",
    "\"¿Qué me recomiendas comprar en Whole Foods?\"  \n",
    "\n",
    "Respuesta (Personalizada):  \n",
    "\"Como disfrutas Whole Foods, podrías probar sus sopas orgánicas o bocadillos saludables. ¿Has probado su sopa de green peas?\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f70619-400d-43d3-85bd-1eb87a676681",
   "metadata": {},
   "source": [
    "## Cómo ejecutar el código desde Visual Studio Code\n",
    "* En Visual Studio Code, busca el archivo 027-collection-schema.py  \n",
    "* En la terminal, asegúrate de estar en el directorio del archivo y ejecuta:  \n",
    "    * python 027-collection-schema.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f563796-3a4c-45e9-a0a4-e7386458d5db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
