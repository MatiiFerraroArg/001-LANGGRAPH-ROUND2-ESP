{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22720783-41ba-494b-9567-d82d32d5ed6d",
   "metadata": {},
   "source": [
    "# Utilizando complex Memory Schemas para operaciones avanzadas  \n",
    "* En el ejercicio anterior, guardamos la memoria a largo plazo como un string de texto. En este, aprenderemos a usar un esquema más sofisticado y a aprovecharlo para operaciones avanzadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1787e-14c1-428c-bd98-de0088a203a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e336-d054-412c-8a3f-1fbec63e1bcd",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda8d4-80cf-4b8f-9981-94edda5e9911",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 026-profile-schema.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-langgraph**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e766aa-f3e2-491f-be99-d0c6b700d47a",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df978ec5-bfd2-4167-bd33-86bc2687d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel35 = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chatModel4o = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e30df9-0cd0-4098-93ed-4bdabb05b55e",
   "metadata": {},
   "source": [
    "## Definiendo el formato de la memoria de nuestro perfil de usuario (User Profile memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "440470ff-73a1-4398-a919-5bef46c8b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class UserProfile(TypedDict):\n",
    "    \"\"\"User profile schema with typed fields\"\"\"\n",
    "    user_name: str  # The user's preferred name\n",
    "    interests: List[str]  # A list of the user's interests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352fcc3-14f2-4733-a0bc-5225681296f6",
   "metadata": {},
   "source": [
    "## Expliquemos lo que acabamos de hacer\n",
    "\n",
    "Este código define una **estructura de datos personalizada (custom data structure)** llamada `UserProfile` utilizando `TypedDict` de Python. Aquí tienes un desglose en términos sencillos:\n",
    "\n",
    "1. **Propósito**: Describe cómo debe organizarse un perfil de usuario y qué tipo de datos debe contener. Esto ayuda a que el código sea más claro y evita errores.\n",
    "\n",
    "2. **Importación de módulos**:  \n",
    "   - `TypedDict` se importa para crear un diccionario estructurado con tipos específicos para cada valor.  \n",
    "   - `List` se importa para especificar que uno de los campos contendrá una lista de elementos.\n",
    "\n",
    "3. **Definiendo `UserProfile`:**  \n",
    "   - Es una **estructura similar a un diccionario** donde las claves (keys) y sus tipos de valores correspondientes están predefinidos.  \n",
    "   - La clave `user_name` debe contener una **cadena de texto** (`str`).  \n",
    "   - La clave `interests` debe contener una **lista de cadenas de texto** (`List[str]`).\n",
    "\n",
    "4. **Explicación de los comentarios**:  \n",
    "   - Cada campo tiene un comentario que explica su propósito:\n",
    "     - `user_name`: El nombre preferido del usuario.  \n",
    "     - `interests`: Una lista de cosas que le interesan al usuario.\n",
    "\n",
    "#### Ejemplo de uso:\n",
    "```python\n",
    "profile: UserProfile = {\n",
    "    \"user_name\": \"Alice\",\n",
    "    \"interests\": [\"programación\", \"lectura\", \"música\"]\n",
    "}\n",
    "```\n",
    "\n",
    "En este ejemplo:  \n",
    "- `\"user_name\"` es un string de texto, `\"Alice\"`.  \n",
    "- `\"interests\"` es una list de strings de texto, `[\"programación\", \"lectura\", \"música\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8b752-7155-4320-8bcd-66a3a85556b2",
   "metadata": {},
   "source": [
    "## Ahora utilicemos el esquema de perfil de usuario (User Profile schema) anterior para guardar los datos de nuestro primer usuario en la Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec4ee4a-ebd3-46ea-a7b2-22f654fdeb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypedDict instance\n",
    "user_profile: UserProfile = {\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"interests\": [\"vespa\", \"AI\", \"SF\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18bd2336-050b-46e4-9bcc-ebe61de04262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize the in-memory store\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memory\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = \"user_profile\"\n",
    "value = user_profile\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b11c2-e70b-4fec-9211-bc5565665cb0",
   "metadata": {},
   "source": [
    "## Expliquemos el código anterior en términos sencillos\n",
    "\n",
    "El código anterior se basa en el ejemplo de `UserProfile` y muestra cómo **almacenar datos de usuario en memoria** utilizando la clase `InMemoryStore` de LangGraph. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Crear un perfil de usuario\n",
    "```python\n",
    "user_profile: UserProfile = {\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"interests\": [\"vespa\", \"IA\", \"San Francisco\"]\n",
    "}\n",
    "```\n",
    "- Se crea un **diccionario de perfil de usuario** basado en la estructura `UserProfile`.  \n",
    "- Contiene:\n",
    "  - **user_name** = `\"Julio\"`  \n",
    "  - **interests** = `[\"vespa\", \"IA\", \"San Francisco\"]`\n",
    "\n",
    "#### Importar los módulos necesarios\n",
    "```python\n",
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "```\n",
    "- **`uuid`**: Útil para generar identificadores únicos (aún no se usa en este fragmento, pero podría usarse más adelante).  \n",
    "- **`InMemoryStore`**: Una herramienta de LangGraph para **almacenar datos temporalmente** en memoria.\n",
    "\n",
    "#### Inicializar el almacenamiento en memoria\n",
    "```python\n",
    "in_memory_store = InMemoryStore()\n",
    "```\n",
    "- Se crea una **instancia** de `InMemoryStore` para **guardar datos en memoria**.\n",
    "\n",
    "#### Definir un namespace para el almacenamiento\n",
    "```python\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memory\")\n",
    "```\n",
    "- **`user_id = \"1\"`**: Representa el ID del usuario (en este caso, Julio).  \n",
    "- **`namespace_for_memory = (user_id, \"memory\")`**: Combina el ID del usuario y una etiqueta (\"memory\") para crear un **namespace único**, permitiendo almacenar los datos organizadamente.\n",
    "\n",
    "#### Guardar datos en la memoria\n",
    "```python\n",
    "key = \"user_profile\"\n",
    "value = user_profile\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "```\n",
    "- **`key = \"user_profile\"`**: Define una **etiqueta** para los datos almacenados.  \n",
    "- **`value = user_profile`**: Contiene el perfil de usuario definido anteriormente.  \n",
    "- **`put()`**: Guarda los datos en memoria utilizando:\n",
    "  - **namespace**: `(user_id, \"memory\")` – Organiza los datos por usuario o contexto.  \n",
    "  - **Par key-value**: `\"user_profile\"` → `{user_name: \"Julio\", interests: [...]}`.\n",
    "\n",
    "#### ¿Qué sucede en la memoria?\n",
    "Los datos se almacenan en la memoria con la siguiente estructura:\n",
    "```json\n",
    "{\n",
    "  (\"1\", \"memory\"): {\n",
    "      \"user_profile\": {\n",
    "          \"user_name\": \"Julio\",\n",
    "          \"interests\": [\"vespa\", \"IA\", \"San Francisco\"]\n",
    "      }\n",
    "  }\n",
    "}\n",
    "```\n",
    "Esto significa que ahora podemos recuperar y utilizar los datos del usuario en la aplicación mientras se mantenga en ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac03eb3-9bdf-42e6-8cc6-81bdcec28407",
   "metadata": {},
   "source": [
    "## OK. Ahora veamos cómo recuperar esta información de la Memory Store.  \n",
    "* Como aprendimos en el ejercicio anterior, podemos usar [search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search) para recuperar objetos del store por namespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783b5ccf-f6b3-4586-9861-2fc5a8e60ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'user_name': 'Julio', 'interests': ['vespa', 'AI', 'SF']}, 'key': 'user_profile', 'namespace': ['1', 'memory'], 'created_at': '2025-01-07T08:48:22.047331+00:00', 'updated_at': '2025-01-07T08:48:22.047333+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Search \n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e639e-cd59-4ed2-8efe-2290a6718ad1",
   "metadata": {},
   "source": [
    "* Como recordarás, también podemos usar [get](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.get) para recuperar un objeto específico mediante el namespace y la key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a893a54-709a-4c9e-96dc-dbe7f524e136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Julio', 'interests': ['vespa', 'AI', 'SF']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the memory by namespace and key\n",
    "profile = in_memory_store.get(namespace_for_memory, \"user_profile\")\n",
    "profile.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc01447-76ff-43e0-a75c-3bc0086af586",
   "metadata": {},
   "source": [
    "## Si usamos bind para vincular el schema de UserProfile con el modelo LLM, nuestra aplicación responderá con structured output (output estructurado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780e6e3b-c9e3-4fe8-bab8-afb70f1b1c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Julio', 'interests': ['driving vespa']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# PAY ATTENTION HERE: see how we use the UserProfile schema\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(UserProfile)\n",
    "\n",
    "# PAY ATTENTION HERE: Using the bound model, now the app \n",
    "# will respond with structured output.\n",
    "# Invoke the model to produce structured output that matches the schema\n",
    "structured_output = model_with_structure.invoke([HumanMessage(\"My name is Julio, I like to drive my vespa.\")])\n",
    "structured_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c585a12-c33a-4cd8-89ce-98bd4ccb4693",
   "metadata": {},
   "source": [
    "## Revisemos lo que acabamos de hacer\n",
    "\n",
    "El código anterior amplía el ejemplo previo y muestra cómo **usar el modelo LLM para procesar texto de entrada (input) y generar datos estructurados** que coincidan con el esquema `UserProfile`. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Importar las bibliotecas necesarias\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "```\n",
    "\n",
    "- **`pydantic`**: Ayuda a definir y validar estructuras de datos (aunque no se usa directamente aquí, es compatible con el schema).  \n",
    "- **`HumanMessage`**: Representa un mensaje escrito por un humano, simulando la entrada (input) del usuario.  \n",
    "- **`ChatOpenAI`**: Conecta con el modelo GPT de OpenAI para generar respuestas.\n",
    "\n",
    "#### Inicializar el modelo de chat\n",
    "```python\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "```\n",
    "- **`ChatOpenAI`**: Inicializa un modelo de IA (GPT-4o).  \n",
    "- **`model=\"gpt-4o\"`**: Especifica la versión del modelo de IA a utilizar.  \n",
    "- **`temperature=0`**: Controla el nivel de aleatoriedad. 0 significa que el output será **consistente y predecible**.\n",
    "\n",
    "#### Vincular el esquema al modelo\n",
    "```python\n",
    "model_with_structure = model.with_structured_output(UserProfile)\n",
    "```\n",
    "- **Propósito**: Asegura que la salida (output) del modelo coincida con el schema **`UserProfile`** definido previamente.  \n",
    "- **Idea clave**: Obliga a la IA a **formatear su respuesta como datos estructurados** (diccionario) con los siguientes campos:  \n",
    "  - `user_name` (cadena de texto).  \n",
    "  - `interests` (lista de cadenas de texto).\n",
    "\n",
    "#### Procesar la entrada (input) del usuario\n",
    "```python\n",
    "structured_output = model_with_structure.invoke(\n",
    "    [HumanMessage(\"Me llamo Julio, me gusta conducir mi vespa.\")]\n",
    ")\n",
    "```\n",
    "- **Entrada**: Simula un mensaje humano: `\"Me llamo Julio, me gusta conducir mi vespa.\"`  \n",
    "- **`invoke()`**: Procesa la entrada con el modelo LLM y genera datos estructurados según el esquema.  \n",
    "- **Salida estructurada**: El modelo LLM analiza el texto y devuelve la información en el formato requerido.\n",
    "\n",
    "#### ¿Cómo se ve el output?\n",
    "El output se formatea como un **diccionario válido de `UserProfile`**, por ejemplo:\n",
    "```python\n",
    "{\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"interests\": [\"vespa\"]\n",
    "}\n",
    "```\n",
    "\n",
    "#### ¿Cómo funciona?\n",
    "1. **El modelo LLM procesa el mensaje**:  \n",
    "   - Extrae el nombre (\"Julio\").  \n",
    "   - Identifica el interés (\"vespa\").  \n",
    "\n",
    "2. **Validación del esquema**:  \n",
    "   - Asegura que el output coincida con la estructura de `UserProfile`.  \n",
    "   - Devuelve datos estructurados en lugar de texto sin formato.\n",
    "\n",
    "#### Caso de uso práctico\n",
    "Imagina que estás construyendo un chatbot que recopila perfiles de usuario para ofrecer recomendaciones. En lugar de analizar manualmente los mensajes de los usuarios, el modelo LLM extrae y estructura la información automáticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9e419-51f9-4a99-9e91-d44e3134d310",
   "metadata": {},
   "source": [
    "## Ahora podemos agregar este cambio al chatbot con memoria a largo plazo que construimos en el ejercicio anterior  \n",
    "* **PRESTA ATENCIÓN**: observa cómo usamos la variable `new_memory` a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe43631-56f8-45c0-abfe-1d3e65f7f94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"Create or update a user profile memory based on the user's chat history. \n",
    "This will be saved for long-term memory. If there is an existing memory, simply update it. \n",
    "Here is the existing memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve existing memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "        \n",
    "    # Format the existing memory in the instruction\n",
    "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "\n",
    "    # PAY ATTENTION: here is where we define the new_memory variable.\n",
    "    # Invoke the model to produce structured output that matches the schema\n",
    "    new_memory = model_with_structure.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "\n",
    "    # Overwrite the existing use profile memory\n",
    "    key = \"user_memory\"\n",
    "    store.put(namespace, key, new_memory)\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832c81c-6218-44e4-ab95-5ec8d3241292",
   "metadata": {},
   "source": [
    "## Este código es muy similar al que usamos en el último ejercicio  \n",
    "\n",
    "El código anterior crea un **chatbot con memoria** utilizando LangGraph, el modelo LLM de OpenAI y almacenamiento estructurado de memoria. Garantiza que el chatbot pueda **recordar información del usuario** y **actualizar su memoria** en función de nuevas conversaciones. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Importar las bibliotecas necesarias\n",
    "```python\n",
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "```\n",
    "- Se importan bibliotecas para **gestionar la memoria**, **flujos de trabajo basados en graphs** y **manejo de mensajes**.  \n",
    "- `Image` y `display` se utilizan para **visualizar el flujo de trabajo** más adelante.\n",
    "\n",
    "#### Mensajes del sistema e instrucciones\n",
    "```python\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory... {memory}\"\"\"\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"Create or update a user profile memory... {memory}\"\"\"\n",
    "```\n",
    "- Definen los **prompts del sistema** para el modelo LLM.  \n",
    "- **`MODEL_SYSTEM_MESSAGE`**: Indica al chatbot que **use la memoria** (si está disponible) para **personalizar respuestas**.  \n",
    "- **`CREATE_MEMORY_INSTRUCTION`**: Indica al chatbot que **actualice la memoria** después de cada interacción.\n",
    "\n",
    "#### Función de respuesta del chatbot\n",
    "```python\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "Esta función genera **respuestas** para el chatbot:\n",
    "\n",
    "**1. Recuperar la memoria del usuario**\n",
    "```python\n",
    "user_id = config[\"configurable\"][\"user_id\"]\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = store.get(namespace, \"user_memory\")\n",
    "```\n",
    "- Obtiene la memoria del usuario (si existe) según su **ID**.\n",
    "\n",
    "**2. Formatear la memoria**\n",
    "```python\n",
    "formatted_memory = (\n",
    "    f\"Nombre: {memory_dict.get('user_name', 'Desconocido')}\\n\"\n",
    "    f\"Intereses: {', '.join(memory_dict.get('interests', []))}\"\n",
    ")\n",
    "```\n",
    "- Prepara la memoria (nombre e intereses) como **resumen de texto** para la IA.\n",
    "\n",
    "**3. Generar respuesta**\n",
    "```python\n",
    "response = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "```\n",
    "- Combina la memoria con el historial de chat y **activa el modelo de IA** para generar una respuesta personalizada.\n",
    "\n",
    "#### Función para actualizar la memoria\n",
    "```python\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "Esta función **actualiza la memoria** después de procesar un mensaje del usuario:\n",
    "\n",
    "1. **Recupera la memoria existente** de la misma manera que antes.  \n",
    "2. **Formatea la instrucción para actualizar la memoria**\n",
    "```python\n",
    "system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "```\n",
    "- Prepara instrucciones para que el modelo LLM **actualice la memoria** con base en la conversación más reciente.  \n",
    "3. **Ejecuta el modelo para actualizar la memoria**\n",
    "```python\n",
    "new_memory = model_with_structure.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "```\n",
    "- El modelo LLM actualiza la memoria (por ejemplo, agrega nuevos intereses).  \n",
    "4. **Guarda la memoria actualizada**\n",
    "```python\n",
    "store.put(namespace, key, new_memory)\n",
    "```\n",
    "- Sobrescribe la **memoria del usuario** en el almacenamiento.\n",
    "\n",
    "#### Definir el flujo de trabajo graphs**\n",
    "```python\n",
    "builder = StateGraph(MessagesState)\n",
    "```\n",
    "- Define un **flujo de trabajo basado en graphs** para controlar el proceso del chatbot paso a paso.\n",
    "\n",
    "#### Nodos y edges**\n",
    "```python\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "```\n",
    "- **Nodos**: Representan pasos en el proceso (generar respuesta → actualizar memoria).  \n",
    "- **Edges**: Definen la **secuencia** de pasos.  \n",
    "  - Inicio → Responder → Actualizar Memoria → Fin.\n",
    "\n",
    "#### Configurar la memoria**\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "within_thread_memory = MemorySaver()\n",
    "```\n",
    "- **`across_thread_memory`**: Memoria a largo plazo (persiste entre sesiones).  \n",
    "- **`within_thread_memory`**: Memoria a corto plazo (se borra al finalizar la conversación).\n",
    "\n",
    "#### Compilar y visualizar el workflow**\n",
    "```python\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "```\n",
    "- **Compila** el graph con los sistemas de memoria adjuntos.  \n",
    "- **Visualiza** el flujo de trabajo como un **diagrama** para ver cómo se conectan los steps.\n",
    "\n",
    "#### Ejemplo de interacción**\n",
    "1. **Usuario:** `\"Me llamo Julio y me gusta la IA.\"`  \n",
    "2. **Chatbot:** `\"¡Hola Julio! La IA es fascinante. ¿Qué más te interesa?\"`  \n",
    "3. **Actualización de memoria:** Guarda `\"Julio\"` como el nombre y agrega `\"IA\"` a la lista de intereses.  \n",
    "4. **Siguiente interacción:** `\"Háblame sobre los robots.\"`  \n",
    "5. **Chatbot:** `\"Dado que te gusta la IA, es posible que también disfrutes hablando sobre robots.\"`  \n",
    "\n",
    "Este sistema permite que el chatbot **recuerde conversaciones previas** y **personalice las respuestas** de manera más natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9deb3a-7705-46a0-9aac-dc6a79700568",
   "metadata": {},
   "source": [
    "## Probemos este nuevo chatbot y veamos cómo guarda la información del usuario en la memoria del perfil de usuario (User Profile memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe5af33-7f7c-42a0-a360-efb5d982006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Julio and I like to drive my vespa around San Francisco and eat at Whole Foods.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Julio! That sounds like a lot of fun. San Francisco is a great city to explore on a Vespa, and Whole Foods has some delicious options. Do you have any favorite spots or foods you like to get there?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio and I like to drive my vespa around San Francisco and eat at Whole Foods.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41cb8f05-d5b0-4a51-a818-0c7286e9c790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Julio',\n",
       " 'interests': ['driving Vespa',\n",
       "  'exploring San Francisco',\n",
       "  'eating at Whole Foods']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb351a52-1d7c-48c0-96da-f9c630efe4e7",
   "metadata": {},
   "source": [
    "## Revisemos lo que acabamos de hacer  \n",
    "\n",
    "El código anterior demuestra cómo **ejecutar el chatbot con memoria** e **inspeccionar la memoria actualizada del usuario** después de procesar la entrada (el input). Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Configurar los ajustes de memoria  \n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "```\n",
    "- **`thread_id`**: Rastrea la **memoria a corto plazo** para la sesión actual.  \n",
    "- **`user_id`**: Rastrea la **memoria a largo plazo** que persiste entre sesiones.  \n",
    "- Estos IDs se utilizan para **organizar la memoria** por usuario y sesión.\n",
    "\n",
    "#### Proporcionar entrada del usuario  \n",
    "```python\n",
    "input_messages = [HumanMessage(\n",
    "    content=\"Hola, me llamo Julio y me gusta conducir mi vespa por San Francisco y comer en Whole Foods.\"\n",
    ")]\n",
    "```\n",
    "- **Simula un mensaje del usuario**: Julio se presenta y comparte sus intereses.  \n",
    "- **`HumanMessage`**: Marca la entrada (el input) como un mensaje del usuario.\n",
    "\n",
    "#### Procesar el input en el graph\n",
    "```python\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "- **Procesa el input paso a paso** a través del **flujo de trabajo del graph** (definido previamente).  \n",
    "- **Flujo de procesamiento**:  \n",
    "  1. **Call Model**: Usa la memoria para personalizar la respuesta.  \n",
    "  2. **Write Memory**: Actualiza la memoria a largo plazo con los nuevos detalles.  \n",
    "- **Imprimir la respuesta**:\n",
    "  - Muestra la **respuesta generada por la IA (el modelo LLM)** después de analizar el input del usuario.  \n",
    "  - **`pretty_print()`**: Formatea la salida para que sea más legible.\n",
    "\n",
    "#### Recuperar la memoria actualizada\n",
    "```python\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.value\n",
    "```\n",
    "- **Accede a la memoria guardada**:  \n",
    "  - **`across_thread_memory`**: Almacena la memoria a largo plazo entre sesiones.  \n",
    "  - **`namespace`**: Recupera la memoria asociada al **usuario con ID = \"1\"**.  \n",
    "  - **`get`**: Recupera los datos almacenados bajo la clave `\"user_memory\"`.  \n",
    "\n",
    "- **Inspeccionar la memoria**:\n",
    "  - Muestra el perfil actualizado almacenado en la memoria según la entrada del usuario.  \n",
    "\n",
    "#### Ejemplo de salida: contenido de la memoria\n",
    "```python\n",
    "{\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"interests\": [\"vespa\", \"San Francisco\", \"Whole Foods\"]\n",
    "}\n",
    "```\n",
    "- **Actualización de memoria**:  \n",
    "  - La IA (el modelo LLM) ha:\n",
    "    1. Extraído el **nombre** (\"Julio\").  \n",
    "    2. Agregado los **intereses**: `\"vespa\"`, `\"San Francisco\"` y `\"Whole Foods\"`.  \n",
    "\n",
    "#### Características clave\n",
    "1. **Personalización**: El chatbot usa la memoria almacenada para generar **respuestas con contexto**.  \n",
    "2. **Aprendizaje de conversaciones**: La memoria se actualiza con **nueva información** a medida que avanza el chat.  \n",
    "3. **Persistencia de memoria**: La memoria a largo plazo permite que el chatbot **recuerde detalles** incluso después de reiniciar.  \n",
    "4. **Output en tiempo real**: Las respuestas se generan y muestran **en flujo**, paso a paso.  \n",
    "\n",
    "#### Ejemplo de interacción\n",
    "1. **Entrada del usuario**  \n",
    "   `\"Hola, me llamo Julio y me gusta conducir mi vespa por San Francisco y comer en Whole Foods.\"`\n",
    "\n",
    "2. **Respuesta del chatbot**  \n",
    "   `\"¡Hola Julio! Qué genial que disfrutes conducir tu vespa por San Francisco. Whole Foods es un excelente lugar para comer.\"`\n",
    "\n",
    "3. **Verificación de memoria**  \n",
    "   **Antes del chat:**  \n",
    "   ```\n",
    "   {}\n",
    "   ```\n",
    "   **Después del chat:**  \n",
    "   ```\n",
    "   {\n",
    "       \"user_name\": \"Julio\",\n",
    "       \"interests\": [\"vespa\", \"San Francisco\", \"Whole Foods\"]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "#### Conclusión  \n",
    "Este sistema crea un **chatbot inteligente** que puede **aprender y recordar las preferencias del usuario** con el tiempo. Procesa el input del usuario, **actualiza su memoria** y **recupera la información almacenada** cuando es necesario, haciendo que las conversaciones sean **personalizadas y consistentes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73cb2f0-bfd4-41f3-acf7-0763de7e2dc9",
   "metadata": {},
   "source": [
    "## Eso funcionó bien. Veamos cómo funciona la aplicación anterior si definimos un schema de memoria (Memory Schema) más complejo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "459eb499-b9cf-4d87-8af9-0fb58425c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "class OutputFormat(BaseModel):\n",
    "    preference: str\n",
    "    sentence_preference_revealed: str\n",
    "\n",
    "class TelegramPreferences(BaseModel):\n",
    "    preferred_encoding: Optional[List[OutputFormat]] = None\n",
    "    favorite_telegram_operators: Optional[List[OutputFormat]] = None\n",
    "    preferred_telegram_paper: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class MorseCode(BaseModel):\n",
    "    preferred_key_type: Optional[List[OutputFormat]] = None\n",
    "    favorite_morse_abbreviations: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class Semaphore(BaseModel):\n",
    "    preferred_flag_color: Optional[List[OutputFormat]] = None\n",
    "    semaphore_skill_level: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class TrustFallPreferences(BaseModel):\n",
    "    preferred_fall_height: Optional[List[OutputFormat]] = None\n",
    "    trust_level: Optional[List[OutputFormat]] = None\n",
    "    preferred_catching_technique: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class CommunicationPreferences(BaseModel):\n",
    "    telegram: TelegramPreferences\n",
    "    morse_code: MorseCode\n",
    "    semaphore: Semaphore\n",
    "\n",
    "class UserPreferences(BaseModel):\n",
    "    communication_preferences: CommunicationPreferences\n",
    "    trust_fall_preferences: TrustFallPreferences\n",
    "\n",
    "# PAY ATTENTION HERE: this is the complex schema we will use\n",
    "class TelegramAndTrustFallPreferences(BaseModel):\n",
    "    pertinent_user_preferences: UserPreferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d3787cd-2bc1-4a63-bcc6-329a160a949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for TelegramAndTrustFallPreferences\n",
      "pertinent_user_preferences.communication_preferences.semaphore\n",
      "  Input should be a valid dictionary or instance of Semaphore [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n"
     ]
    }
   ],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(TelegramAndTrustFallPreferences)\n",
    "\n",
    "# Conversation\n",
    "conversation = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "Operator: Certainly. Morse code or standard encoding?\n",
    "Customer: Morse, please. I love using a straight key.\n",
    "Operator: Excellent. What's your message?\n",
    "Customer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\n",
    "Operator: Done. Shall I use our \"Daredevil\" paper for this daring message?\n",
    "Customer: Perfect! Send it by your fastest carrier pigeon.\n",
    "Operator: It'll be there within the hour, sir.\"\"\"\n",
    "\n",
    "# PAY ATTENTION: do not get distracted by the <convo> tag here. We will explain it below.\n",
    "# Invoke the model\n",
    "try:\n",
    "    model_with_structure.invoke(f\"\"\"Extract the preferences from the following conversation:\n",
    "    <convo>\n",
    "    {conversation}\n",
    "    </convo>\"\"\")\n",
    "except ValidationError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ded8db-db98-473a-98e1-d4e951e11e3e",
   "metadata": {},
   "source": [
    "## ¡Ups! Eso no ha funcionado muy bien. ¿Qué ha pasado?\n",
    "\n",
    "El código anterior intenta **extraer preferencias estructuradas** sobre métodos de comunicación y preferencias en \"trust falls\" (ejercicios de confianza en grupo) a partir de una conversación dada. Utiliza **modelos de Pydantic** para definir la estructura de los datos y luego **vincula esa estructura** a un modelo de IA (un modelo LLM) para garantizar que el output siga el formato esperado. Sin embargo, **falla debido a un error de validación**. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Definición de Data Models (Schemas)\n",
    "\n",
    "**Data Model (Schema) básico: `OutputFormat`**\n",
    "```python\n",
    "class OutputFormat(BaseModel):\n",
    "    preference: str\n",
    "    sentence_preference_revealed: str\n",
    "```\n",
    "- Define la **estructura de las preferencias** con dos campos obligatorios:\n",
    "  - **`preference`**: Describe la preferencia del usuario.  \n",
    "  - **`sentence_preference_revealed`**: Almacena la frase en la que se mencionó la preferencia.  \n",
    "\n",
    "#### Preferencias de comunicación\n",
    "Los modelos **`TelegramPreferences`**, **`MorseCode`** y **`Semaphore`** definen preferencias para **diferentes métodos de comunicación** (telegrama, código Morse, Semaphore).  \n",
    "- Cada modelo incluye **listas opcionales** de objetos `OutputFormat`.\n",
    "\n",
    "Ejemplo:\n",
    "```python\n",
    "class MorseCode(BaseModel):\n",
    "    preferred_key_type: Optional[List[OutputFormat]] = None\n",
    "    favorite_morse_abbreviations: Optional[List[OutputFormat]] = None\n",
    "```\n",
    "- Almacena preferencias sobre **tipos de keys en Morse** y **abreviaciones favoritas**.  \n",
    "- Estas propiedades son **opcionales** (pueden estar ausentes o ser `None`).\n",
    "\n",
    "#### Preferencias en Trust Fall**\n",
    "```python\n",
    "class TrustFallPreferences(BaseModel):\n",
    "    preferred_fall_height: Optional[List[OutputFormat]] = None\n",
    "    trust_level: Optional[List[OutputFormat]] = None\n",
    "    preferred_catching_technique: Optional[List[OutputFormat]] = None\n",
    "```\n",
    "- Define preferencias en **Trust Fall**, como:\n",
    "  - Fall height.  \n",
    "  - Trust level.  \n",
    "  - Catching technique.  \n",
    "\n",
    "#### Estructuras anidadas\n",
    "```python\n",
    "class CommunicationPreferences(BaseModel):\n",
    "    telegram: TelegramPreferences\n",
    "    morse_code: MorseCode\n",
    "    semaphore: Semaphore\n",
    "```\n",
    "- Combina todos los métodos de comunicación en un solo modelo.\n",
    "\n",
    "```python\n",
    "class UserPreferences(BaseModel):\n",
    "    communication_preferences: CommunicationPreferences\n",
    "    trust_fall_preferences: TrustFallPreferences\n",
    "```\n",
    "- Une **preferencias de comunicación** y **preferencias en trust falls**.\n",
    "\n",
    "```python\n",
    "class TelegramAndTrustFallPreferences(BaseModel):\n",
    "    pertinent_user_preferences: UserPreferences\n",
    "```\n",
    "- **Modelo final** que encapsula todas las preferencias en una sola estructura.\n",
    "\n",
    "#### Vincular (bind) con el modelo LLM\n",
    "```python\n",
    "model_with_structure = model.with_structured_output(TelegramAndTrustFallPreferences)\n",
    "```\n",
    "- Obliga a la IA a generar un output que coincida con la estructura **`TelegramAndTrustFallPreferences`**.  \n",
    "- Asegura que la respuesta siga el **esquema de datos anidado**.\n",
    "\n",
    "#### ¿Dónde ocurrió el error?\n",
    "\n",
    "#### Input del usuario\n",
    "```python\n",
    "conversation = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "...\"\"\"\n",
    "```\n",
    "- La conversación menciona preferencias como:\n",
    "  - Tipo de tecla Morse: **\"straight key\"**.  \n",
    "  - Altura de caída: **\"higher fall\"**.  \n",
    "  - Técnica de captura: **\"diamond formation\"**.  \n",
    "  - Tipo de papel: **\"Daredevil paper\"**.  \n",
    "\n",
    "#### Error al invocar el modelo\n",
    "```python\n",
    "1 validation error for TelegramAndTrustFallPreferences\n",
    "pertinent_user_preferences.communication_preferences.semaphore\n",
    "  Input should be a valid dictionary or instance of Semaphore [type=model_type, input_value=None, input_type=NoneType]\n",
    "```\n",
    "\n",
    "#### ¿Qué pasó?\n",
    "- La IA (el modelo lLM) **no generó datos** para el campo **`semaphore`** en `communication_preferences`.  \n",
    "- En su lugar, devolvió **`None`**, pero el esquema **requiere un objeto válido** de tipo **`Semaphore`**, incluso si todos sus campos están vacíos.\n",
    "\n",
    "#### ¿Por qué ocurrió?\n",
    "- **El schema espera que el campo `semaphore` exista** en el output, incluso si no tiene datos.  \n",
    "- **Pero la IA (el modelo lLM) omitió por completo este campo**, devolviendo `None`, lo que no es un objeto válido según el modelo `Semaphore`.  \n",
    "\n",
    "#### Soluciones posibles\n",
    "1. **Permitir `None` como valor válido** en el esquema:\n",
    "   ```python\n",
    "   class CommunicationPreferences(BaseModel):\n",
    "       telegram: Optional[TelegramPreferences] = None\n",
    "       morse_code: Optional[MorseCode] = None\n",
    "       semaphore: Optional[Semaphore] = None  # Permitir que sea None\n",
    "   ```\n",
    "   - Esto evitaría la validación estricta y aceptaría `None`.\n",
    "\n",
    "2. **Modificar la instrucción a la IA** para que siempre genere una estructura completa, incluso con valores vacíos:\n",
    "   ```python\n",
    "   \"Ensure that all fields are present in the response, even if they contain empty lists or null values.\"\n",
    "   ```\n",
    "   - Forzar a la IA a incluir `semaphore` en la respuesta.\n",
    "\n",
    "3. **Agregar valores predeterminados en los modelos**:\n",
    "   ```python\n",
    "   class Semaphore(BaseModel):\n",
    "       flag_positions: List[OutputFormat] = []\n",
    "   ```\n",
    "   - Así, `semaphore` nunca será `None`, sino al menos una estructura vacía.\n",
    "\n",
    "#### Conclusión\n",
    "El problema surgió porque **la IA (el modelo LLM) omitió un campo que el schema esperaba explícitamente**. La solución es:\n",
    "- **Permitir valores `None` en la estructura**,  \n",
    "- **Forzar a la IA a devolver una estructura completa**,  \n",
    "- **O definir valores predeterminados en el esquema**.  \n",
    "\n",
    "¡Ahora podemos ajustar el código y hacer que la memoria estructurada funcione correctamente!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f321da6b-15d9-4a73-8121-3e572a84284a",
   "metadata": {},
   "source": [
    "## ¿Qué está haciendo la etiqueta `<convo>` en el código anterior?\n",
    "\n",
    "En el código anterior, la etiqueta `<convo>` se usa para envolver el texto de la conversación antes de pasarlo al modelo. Sin embargo, **no es una característica predefinida de LangGraph o Pydantic; simplemente es una convención definida por el desarrollador para estructurar la entrada de texto.**\n",
    "\n",
    "#### ¿Qué hace `<convo>` en este código?\n",
    "1. **Encapsulación**: Actúa como un contenedor que delimita la conversación dentro del texto de entrada.  \n",
    "2. **Pista contextual para el modelo**: Ayuda a que el modelo identifique que el texto dentro de la etiqueta es un diálogo del cual debe extraer preferencias estructuradas.  \n",
    "3. **No es procesado especialmente por LangGraph**: LangGraph no interpreta `<convo>`, simplemente forma parte del formato del prompt.  \n",
    "4. **Uso potencial como XML-like**: Algunos modelos de lenguaje pueden reconocer etiquetas estilo XML (`<convo>` en este caso) y utilizarlas para estructurar mejor la información y extraer datos con mayor precisión.  \n",
    "\n",
    "#### ¿Por qué usar `<convo>`?\n",
    "- Separa visualmente la conversación de la instrucción (`Extrae las preferencias de la siguiente conversación:`).  \n",
    "- Si el modelo ha sido entrenado para reconocer estructuras similares a XML, esto puede mejorar la precisión en el análisis.  \n",
    "- Evita confusiones al procesar textos de múltiples líneas dentro del pipeline de LLM.  \n",
    "#### ¿Qué sucede cuando se invoca el modelo?\n",
    "1. El **texto de la conversación** (envuelto en `<convo>`) se envía a `model_with_structure.invoke(...)`.  \n",
    "2. El **modelo estructurado (`TelegramAndTrustFallPreferences`)** espera que la IA extraiga preferencias del usuario de la conversación.  \n",
    "3. El **LLM procesa el diálogo** y mapea la información a la estructura esperada.  \n",
    "4. Si la salida generada no coincide con el esquema de Pydantic, se genera un **`ValidationError`**.  \n",
    "\n",
    "#### Ejemplo de extracción esperada\n",
    "A partir de la conversación dada, el modelo podría extraer:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"pertinent_user_preferences\": {\n",
    "    \"communication_preferences\": {\n",
    "      \"telegram\": {\n",
    "        \"preferred_telegram_paper\": [\n",
    "          {\n",
    "            \"preference\": \"Daredevil\",\n",
    "            \"sentence_preference_revealed\": \"Shall I use our 'Daredevil' paper for this daring message?\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"morse_code\": {\n",
    "        \"preferred_key_type\": [\n",
    "          {\n",
    "            \"preference\": \"straight key\",\n",
    "            \"sentence_preference_revealed\": \"Morse, please. I love using a straight key.\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    },\n",
    "    \"trust_fall_preferences\": {\n",
    "      \"preferred_fall_height\": [\n",
    "        {\n",
    "          \"preference\": \"higher fall\",\n",
    "          \"sentence_preference_revealed\": \"Tell him I'm ready for a higher fall.\"\n",
    "        }\n",
    "      ],\n",
    "      \"preferred_catching_technique\": [\n",
    "        {\n",
    "          \"preference\": \"diamond formation\",\n",
    "          \"sentence_preference_revealed\": \"I prefer the diamond formation for catching.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Conclusión\n",
    "- `<convo>` **no es** una función especial de LangGraph o Pydantic.  \n",
    "- Es **una convención del desarrollador** para estructurar la conversación dentro del prompt.  \n",
    "- **Facilita la extracción de preferencias** al delimitar claramente el bloque de conversación.  \n",
    "- **El esquema de Pydantic valida** que la salida generada por la IA cumpla con la estructura esperada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca548e-e552-4dcf-b33c-9d90673c67d6",
   "metadata": {},
   "source": [
    "## Solucionando estos problemas con TrustCall\n",
    "* En el código anterior, adoptamos el enfoque de regenerar el schema de **Profile** desde cero cada vez que elegíamos guardar una nueva memoria. Esto es ineficiente, ya que potencialmente desperdicia tokens si el schema contiene mucha información que debe regenerarse cada vez. Además, podríamos perder información al regenerar el perfil desde cero.  \n",
    "* Además, los schemas complejos pueden ser difíciles de extraer.  \n",
    "* Podemos resolver muchos de estos problemas utilizando la biblioteca de código abierto [TrustCall](https://github.com/hinthornw/trustcall), desarrollada por uno de los miembros del equipo de LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad71ba-aa72-4f9c-a5fd-4f9ba3abc979",
   "metadata": {},
   "source": [
    "## Primero, veamos cómo funciona TrustCall con un ejemplo sencillo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d965a98-cd75-46a4-8855-5a2f185cfece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAY ATTENTION: this is the conversation variable we will use below.\n",
    "conversation = [HumanMessage(content=\"Hi, I'm Julio.\"), \n",
    "                AIMessage(content=\"Nice to meet you, Julio.\"), \n",
    "                HumanMessage(content=\"I am interested in Gen AI startups.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2109f58e-1630-4c9a-9fad-df8f356d67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAY ATTENTION: here is where we import create_extractor from truscall\n",
    "from trustcall import create_extractor\n",
    "\n",
    "# Schema \n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\"User profile schema with typed fields\"\"\"\n",
    "    user_name: str = Field(description=\"The user's preferred name\")\n",
    "    interests: List[str] = Field(description=\"A list of the user's interests\")\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Create the extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\"\n",
    ")\n",
    "\n",
    "# Instruction\n",
    "system_msg = \"Extract the user profile from the following conversation\"\n",
    "\n",
    "# PAY ATTENTION: See how we use the extractor and compare it with the previous approach.\n",
    "# Invoke the extractor\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+conversation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caa957da-eca9-40cd-a4a0-1a4fd4eb3487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UserProfile (call_B0RGcNrAOMYuWsgw2oYn2Gta)\n",
      " Call ID: call_B0RGcNrAOMYuWsgw2oYn2Gta\n",
      "  Args:\n",
      "    user_name: Julio\n",
      "    interests: ['Gen AI startups']\n"
     ]
    }
   ],
   "source": [
    "for m in result[\"messages\"]: \n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5153ec-cda0-4e56-9f1d-edf8bda22796",
   "metadata": {},
   "source": [
    "## Veamos qué acabamos de hacer\n",
    "\n",
    "Este ejemplo demuestra cómo usar **TrustCall**, una biblioteca diseñada para **simplificar la extracción de schemas** en conversaciones, haciéndola más **eficiente** y **fiable** que regenerar perfiles desde cero en cada ocasión. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### El problema con los enfoques anteriores\n",
    "- Los métodos anteriores requerían regenerar **todo el schema del perfil** cada vez que el chatbot actualizaba la memoria.  \n",
    "- **Problemas:**\n",
    "  1. **Ineficiencia**: Se desperdician tokens al regenerar todos los datos, incluso las partes que no han cambiado.  \n",
    "  2. **Riesgo de pérdida de datos**: Información importante podría ser **sobrescrita** o perdida.  \n",
    "  3. **Schemas complejos**: Extraer información de schemas grandes y anidados es difícil y propenso a errores.  \n",
    "\n",
    "**Solución**: **TrustCall**, una herramienta para **actualizar campos específicos** en schemas sin regenerar toda la estructura.\n",
    "\n",
    "#### Ejemplo de TrustCall\n",
    "\n",
    "**Paso 1: Datos de la conversación**\n",
    "```python\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hola, soy Julio.\"), \n",
    "    AIMessage(content=\"Encantado de conocerte, Julio.\"), \n",
    "    HumanMessage(content=\"Estoy interesado en startups de IA Generativa.\")\n",
    "]\n",
    "```\n",
    "- **Simulación de chat**:\n",
    "  - Julio se presenta y menciona su interés en **startups de IA Generativa**.  \n",
    "- Estos datos serán **procesados para extraer información del usuario**.\n",
    "\n",
    "\n",
    "**Paso 2: Definir el schema**\n",
    "```python\n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\"Esquema de perfil de usuario con campos tipados\"\"\"\n",
    "    user_name: str = Field(description=\"El nombre preferido del usuario\")\n",
    "    interests: List[str] = Field(description=\"Una lista de intereses del usuario\")\n",
    "```\n",
    "- Define la **estructura de datos** para el perfil del usuario:\n",
    "  - **`user_name`**: Almacena el nombre del usuario (string de texto).  \n",
    "  - **`interests`**: Almacena una **list de intereses** (permite múltiples valores).  \n",
    "- Cada campo tiene una **descripción** para ayudar a la IA (el modelo LLM) a comprender qué datos debe asignar.\n",
    "\n",
    "\n",
    "**Paso 3: Inicializar el modelo**\n",
    "```python\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "```\n",
    "- Se usa **GPT-4o** como modelo de IA.  \n",
    "- **`temperature=0`** garantiza **salidas consistentes**.\n",
    "\n",
    "**Paso 4: Crear el extractor**\n",
    "```python\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],  # Esquema para la extracción\n",
    "    tool_choice=\"UserProfile\"  # Se enfoca solo en este esquema\n",
    ")\n",
    "```\n",
    "- Se crea el **Extractor de TrustCall** para **analizar y extraer datos** según el esquema definido.  \n",
    "- **`tools`**: Especifica el esquema a utilizar (`UserProfile`).  \n",
    "- **`tool_choice`**: Asegura que la IA (el modelo LLM) **solo use este esquema**, evitando datos no relacionados.\n",
    "\n",
    "**Paso 5: Definir instrucciones**\n",
    "```python\n",
    "system_msg = \"Extrae el perfil del usuario de la siguiente conversación\"\n",
    "```\n",
    "- Proporciona una **instrucción clara** para extraer **información del usuario** desde el chat.  \n",
    "- Mantiene la IA (el modelo LLM) **enfocada en la extracción del esquema**, en lugar de respuestas abiertas.\n",
    "\n",
    "\n",
    "**Paso 6: Invocar el extractor**\n",
    "```python\n",
    "result = trustcall_extractor.invoke(\n",
    "    {\"messages\": [SystemMessage(content=system_msg)] + conversation}\n",
    ")\n",
    "```\n",
    "- **Procesa la conversación** usando el esquema y la instrucción.  \n",
    "- **Extrae los campos relevantes** (por ejemplo, nombre e intereses) dentro de la estructura `UserProfile`.  \n",
    "- Automáticamente **rellena solo los campos necesarios** sin sobrescribir datos existentes.\n",
    "\n",
    "\n",
    "**Paso 7: Mostrar resultados**\n",
    "```python\n",
    "for m in result[\"messages\"]: \n",
    "    m.pretty_print()\n",
    "```\n",
    "- Imprime los **datos extraídos** en un formato estructurado y **legible**.\n",
    "\n",
    "#### Output de ejemplo\n",
    "```\n",
    "{\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"interests\": [\"startups de IA Generativa\"]\n",
    "}\n",
    "```\n",
    "- Datos extraídos:\n",
    "  - **`user_name`**: \"Julio\" (desde el primer mensaje del usuario).  \n",
    "  - **`interests`**: [\"startups de IA Generativa\"] (desde el segundo mensaje del usuario).  \n",
    "\n",
    "#### ¿Por qué TrustCall es mejor?\n",
    "\n",
    "1. **Eficiencia**:  \n",
    "   - Solo **actualiza los campos** necesarios, ahorrando tokens y reduciendo costos de API.  \n",
    "\n",
    "2. **Fiabilidad**:  \n",
    "   - Conserva **datos existentes** y evita sobrescrituras accidentales.  \n",
    "\n",
    "3. **Escalabilidad**:  \n",
    "   - Maneja **esquemas complejos** sin necesidad de procesamiento manual.  \n",
    "\n",
    "4. **Flexibilidad**:  \n",
    "   - Funciona con **campos específicos**, permitiendo actualizaciones parciales.  \n",
    "\n",
    "5. **Facilidad de uso**:  \n",
    "   - Requiere una configuración mínima: solo define el schema y deja que el extractor haga el resto.\n",
    "\n",
    "#### Reflexión final\n",
    "\n",
    "Este ejemplo muestra cómo **TrustCall** puede simplificar la **extracción de datos y actualización de memoria** en flujos de trabajo de IA. En lugar de **reconstruir el perfil** cada vez, **solo actualiza las partes relevantes**, haciéndolo más rápido, fiable y adecuado para **schemas complejos**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdac7c-3cec-4ac3-bbc0-363d49bddfa4",
   "metadata": {},
   "source": [
    "## Ahora que estamos más familiarizados con TrustCall, probémoslo con nuestro caso problemático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f072e41a-8a9d-4f67-bc51-5d34e7f9038e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TelegramAndTrustFallPreferences(pertinent_user_preferences=UserPreferences(communication_preferences=CommunicationPreferences(telegram=TelegramPreferences(preferred_encoding=[OutputFormat(preference='Morse', sentence_preference_revealed='Morse code or standard encoding?')], favorite_telegram_operators=None, preferred_telegram_paper=[OutputFormat(preference='Daredevil', sentence_preference_revealed='Shall I use our \"Daredevil\" paper for this daring message?')]), morse_code=MorseCode(preferred_key_type=[OutputFormat(preference='straight key', sentence_preference_revealed='I love using a straight key.')], favorite_morse_abbreviations=None), semaphore=Semaphore(preferred_flag_color=None, semaphore_skill_level=None)), trust_fall_preferences=TrustFallPreferences(preferred_fall_height=[OutputFormat(preference='higher', sentence_preference_revealed=\"I'm ready for a higher fall.\")], trust_level=None, preferred_catching_technique=[OutputFormat(preference='diamond formation', sentence_preference_revealed='I prefer the diamond formation for catching.')])))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PAY ATTENTION: here is where we create the extractor.\n",
    "# See that the variable name \"bound\" is not a very good choice by the LG team.\n",
    "bound = create_extractor(\n",
    "    model,\n",
    "    tools=[TelegramAndTrustFallPreferences],\n",
    "    tool_choice=\"TelegramAndTrustFallPreferences\",\n",
    ")\n",
    "\n",
    "# Conversation\n",
    "conversation = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "Operator: Certainly. Morse code or standard encoding?\n",
    "Customer: Morse, please. I love using a straight key.\n",
    "Operator: Excellent. What's your message?\n",
    "Customer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\n",
    "Operator: Done. Shall I use our \"Daredevil\" paper for this daring message?\n",
    "Customer: Perfect! Send it by your fastest carrier pigeon.\n",
    "Operator: It'll be there within the hour, sir.\"\"\"\n",
    "\n",
    "# PAY ATTENTION: bound is the name of the extractor we created before\n",
    "result = bound.invoke(\n",
    "    f\"\"\"Extract the preferences from the following conversation:\n",
    "<convo>\n",
    "{conversation}\n",
    "</convo>\"\"\"\n",
    ")\n",
    "\n",
    "# Extract the preferences\n",
    "result[\"responses\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276317e-43e3-49e7-9af1-512a64ad8f28",
   "metadata": {},
   "source": [
    "## OK, esta vez nuestro código funcionó. Veamos lo que ha pasado.\n",
    "\n",
    "El código anterior se basa en el ejemplo de **TrustCall** y demuestra cómo usarlo para **extraer preferencias complejas**, específicamente **preferencias de Telegram y Trust Fall**, a partir de una conversación. Aquí tienes una explicación sencilla paso a paso:\n",
    "\n",
    "#### Crear un Extractor con TrustCall\n",
    "```python\n",
    "bound = create_extractor(\n",
    "    model,\n",
    "    tools=[TelegramAndTrustFallPreferences],\n",
    "    tool_choice=\"TelegramAndTrustFallPreferences\",\n",
    ")\n",
    "```\n",
    "- **Propósito**: Configura un **extractor TrustCall** para trabajar con el **schema `TelegramAndTrustFallPreferences`** previamente definido.  \n",
    "- **Parámetros**:  \n",
    "  - **`tools=[TelegramAndTrustFallPreferences]`**: Especifica el esquema a utilizar para la extracción.  \n",
    "  - **`tool_choice=\"TelegramAndTrustFallPreferences\"`**: Asegura que la IA (el modelo LLM) se enfoque solo en este schema, ignorando información no relacionada.  \n",
    "\n",
    "Este paso prepara el extractor para **mapear datos de la conversación** dentro del **schema complejo** con preferencias anidadas.\n",
    "\n",
    "#### Conversación de Entrada\n",
    "```python\n",
    "conversation = \"\"\"Operador: ¿En qué puedo ayudarle con su telegrama, señor?\n",
    "Cliente: Necesito enviar un mensaje sobre nuestro ejercicio de caída de confianza.\n",
    "Operador: Por supuesto. ¿Código Morse o codificación estándar?\n",
    "Cliente: Morse, por favor. Me encanta usar una llave recta.\n",
    "Operador: Excelente. ¿Cuál es su mensaje?\n",
    "Cliente: Dígale que estoy listo para una caída más alta y que prefiero la formación diamante para atraparme.\n",
    "Operador: Listo. ¿Desea que usemos nuestro papel \"Daredevil\" para este audaz mensaje?\n",
    "Cliente: ¡Perfecto! Envíelo con su paloma mensajera más rápida.\n",
    "Operador: Estará allí en menos de una hora, señor.\"\"\"\n",
    "```\n",
    "- Esta es una **conversación simulada** entre un operador y un cliente.  \n",
    "- Contiene **preferencias ocultas** sobre:  \n",
    "  1. **Configuración del telegrama** (tipo de papel, velocidad de entrega).  \n",
    "  2. **Preferencias de caída de confianza** (altura de la caída, técnica de captura).  \n",
    "  3. **Preferencias de código Morse** (tipo de llave).  \n",
    "\n",
    "#### Invocar el Extractor\n",
    "```python\n",
    "result = bound.invoke(\n",
    "    f\"\"\"Extrae las preferencias de la siguiente conversación:\n",
    "<convo>\n",
    "{conversation}\n",
    "</convo>\"\"\"\n",
    ")\n",
    "```\n",
    "- **Propósito**: Envía la conversación al **extractor TrustCall** junto con una instrucción para **extraer las preferencias**.  \n",
    "- **Formato del Mensaje**: Envuelve la conversación dentro de etiquetas similares a XML `<convo>` para **ayudar al modelo a enfocarse** en el contenido relevante.  \n",
    "- **Resultado**: Devuelve las preferencias extraídas en el formato de schema especificado.\n",
    "\n",
    "#### Acceder a las Preferencias Extraídas\n",
    "```python\n",
    "result[\"responses\"][0]\n",
    "```\n",
    "- Recupera la **primera respuesta** del resultado, que contiene las **preferencias estructuradas** extraídas de la conversación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452cac6-3b68-49cb-8bbf-7f83a2ee0fe8",
   "metadata": {},
   "source": [
    "## Hablemos de la respuesta que hemos obtenido\n",
    "\n",
    "El output que hemos recibido es una **representación estructurada** de las preferencias del usuario extraídas de la conversación, organizadas en el schema complejo definido anteriormente. Analicémoslo paso a paso en términos sencillos:\n",
    "\n",
    "#### Schema de Nivel Superior\n",
    "```python\n",
    "TelegramAndTrustFallPreferences(\n",
    "    pertinent_user_preferences=UserPreferences(\n",
    "```\n",
    "- **`TelegramAndTrustFallPreferences`**: El **schema principal** que agrupa todas las preferencias del usuario.  \n",
    "- **`pertinent_user_preferences`**: Contiene las preferencias del usuario organizadas en dos categorías:\n",
    "  1. **Preferencias de Comunicación** (Telegram, Código Morse, Semáforo).  \n",
    "  2. **Preferencias de Caída de Confianza** (altura, técnicas, etc.).\n",
    "\n",
    "#### Preferencias de Comunicación\n",
    "```python\n",
    "communication_preferences=CommunicationPreferences(\n",
    "```\n",
    "Esta sección trata sobre las **preferencias de comunicación**, divididas en tres tipos:\n",
    "\n",
    "**a) Preferencias de Telegram**\n",
    "```python\n",
    "telegram=TelegramPreferences(\n",
    "    preferred_encoding=[\n",
    "        OutputFormat(preference='Morse', \n",
    "        sentence_preference_revealed='Morse code or standard encoding?')\n",
    "    ],\n",
    "    favorite_telegram_operators=None,\n",
    "    preferred_telegram_paper=[\n",
    "        OutputFormat(preference='Daredevil', \n",
    "        sentence_preference_revealed='Shall I use our \"Daredevil\" paper for this daring message?')\n",
    "    ]\n",
    ")\n",
    "```\n",
    "- **Codificación Preferida**:  \n",
    "  - **Preferencia**: \"Morse\".  \n",
    "  - **Evidencia**: Mencionado en: *\"Morse code or standard encoding?\"*  \n",
    "- **Operadores de Telegram Favoritos**:  \n",
    "  - **`None`**—No se proporcionó una preferencia.  \n",
    "- **Papel de Telegram Preferido**:  \n",
    "  - **Preferencia**: \"Daredevil\".  \n",
    "  - **Evidencia**: Mencionado en: *'Shall I use our \"Daredevil\" paper?'*  \n",
    "\n",
    "**b) Preferencias de Código Morse**\n",
    "```python\n",
    "morse_code=MorseCode(\n",
    "    preferred_key_type=[\n",
    "        OutputFormat(preference='straight key', \n",
    "        sentence_preference_revealed='I love using a straight key.')\n",
    "    ],\n",
    "    favorite_morse_abbreviations=None\n",
    ")\n",
    "```\n",
    "- **Tipo de Llave Preferida**:  \n",
    "  - **Preferencia**: \"straight key\".  \n",
    "  - **Evidencia**: Mencionado en: *\"I love using a straight key.\"*  \n",
    "- **Abreviaciones de Código Morse Favoritas**:  \n",
    "  - **`None`**—No se proporcionó una preferencia.\n",
    "\n",
    "**c) Preferencias de Semáforo**\n",
    "```python\n",
    "semaphore=Semaphore(\n",
    "    preferred_flag_color=None, \n",
    "    semaphore_skill_level=None\n",
    ")\n",
    "```\n",
    "- **Color de Bandera Preferido**:  \n",
    "  - **`None`**—No se mencionaron colores.  \n",
    "- **Nivel de Habilidad en Semáforo**:  \n",
    "  - **`None`**—No se mencionó ningún nivel de habilidad.\n",
    "\n",
    "#### Preferencias de Caída de Confianza\n",
    "```python\n",
    "trust_fall_preferences=TrustFallPreferences(\n",
    "    preferred_fall_height=[\n",
    "        OutputFormat(preference='higher', \n",
    "        sentence_preference_revealed=\"I'm ready for a higher fall.\")\n",
    "    ],\n",
    "    trust_level=None,\n",
    "    preferred_catching_technique=[\n",
    "        OutputFormat(preference='diamond formation', \n",
    "        sentence_preference_revealed='I prefer the diamond formation for catching.')\n",
    "    ]\n",
    ")\n",
    "```\n",
    "- **Altura de Caída Preferida**:  \n",
    "  - **Preferencia**: \"higher\".  \n",
    "  - **Evidencia**: Mencionado en: *\"I'm ready for a higher fall.\"*  \n",
    "- **Nivel de Confianza**:  \n",
    "  - **`None`**—No se mencionó explícitamente.  \n",
    "- **Técnica de Captura Preferida**:  \n",
    "  - **Preferencia**: \"diamond formation\".  \n",
    "  - **Evidencia**: Mencionado en: *\"I prefer the diamond formation for catching.\"*  \n",
    "\n",
    "#### Características Clave Observadas en el Output\n",
    "\n",
    "1. **Datos Altamente Estructurados**:  \n",
    "   - Las preferencias están organizadas en categorías distintas.  \n",
    "\n",
    "2. **Seguimiento de Evidencia**:  \n",
    "   - Cada preferencia incluye la **oración de donde fue extraída**, lo que la hace **rastreable**.  \n",
    "\n",
    "3. **Manejo de Valores Faltantes**:  \n",
    "   - Los campos sin datos (por ejemplo, preferencias de semáforo) se establecen en **`None`**, en lugar de generar errores.\n",
    "\n",
    "4. **Soporte para Schemas Complejos**:  \n",
    "   - Los schemas anidados (como `CommunicationPreferences` y `TrustFallPreferences`) se manejan sin problemas.\n",
    "\n",
    "#### 5. Reflexiones Finales\n",
    "- **¿Por qué es Útil este Output?**  \n",
    "  - **Preserva detalles** de la conversación en un **formato estructurado** que puede guardarse en bases de datos o usarse en futuras interacciones.  \n",
    "  - **Evita regenerar perfiles desde cero**, enfocándose solo en **actualizar campos específicos** según sea necesario.  \n",
    "\n",
    "Este enfoque con **TrustCall** simplifica la **extracción de schemas complejos** mientras sigue siendo **escalable y eficiente** para flujos de trabajo impulsados por IA (el modelo LLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fed25-00a5-4eff-a50f-b1896f2e59ba",
   "metadata": {},
   "source": [
    "## OK. Ahora actualicemos nuestro chatbot anterior con memoria a largo plazo agregando TrustCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20e9e620-7af5-4ab4-882d-2c6509437733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Schema \n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\" Profile of a user \"\"\"\n",
    "    user_name: str = Field(description=\"The user's preferred name\")\n",
    "    user_location: str = Field(description=\"The user's location\")\n",
    "    interests: list = Field(description=\"A list of the user's interests\")\n",
    "\n",
    "# Create the extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\", # Enforces use of the UserProfile tool\n",
    ")\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# Extraction instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Create or update the memory (JSON doc) to incorporate information from the following conversation:\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Location: {memory_dict.get('user_location', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"      \n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve existing memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "        \n",
    "    # Get the profile as the value from the list, and convert it to a JSON doc\n",
    "    existing_profile = {\"UserProfile\": existing_memory.value} if existing_memory else None\n",
    "    \n",
    "    # Invoke the extractor\n",
    "    result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)]+state[\"messages\"], \"existing\": existing_profile})\n",
    "    \n",
    "    # Get the updated profile as a JSON object\n",
    "    updated_profile = result[\"responses\"][0].model_dump()\n",
    "\n",
    "    # Save the updated profile\n",
    "    key = \"user_memory\"\n",
    "    store.put(namespace, key, updated_profile)\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8448dd-14e9-4260-a5c5-724e7d94536e",
   "metadata": {},
   "source": [
    "## ¿Cómo estamos usando TrustCall en el código anterior?\n",
    "\n",
    "El código anterior demuestra cómo **TrustCall** se utiliza para **extraer, actualizar y almacenar perfiles de usuario estructurados** en un chatbot con **capacidades de memoria**. Integra **LangGraph** para la gestión del flujo de trabajo y usa **TrustCall** para **extraer datos basados en schemas**. Aquí tienes una explicación sencilla paso a paso:\n",
    "\n",
    "#### Propósito del Código\n",
    "Este chatbot:\n",
    "1. **Extrae información del usuario** (nombre, ubicación e intereses) a partir de conversaciones.  \n",
    "2. **Almacena los datos extraídos** en memoria para **personalizar** futuras interacciones.  \n",
    "3. **Actualiza la memoria de forma incremental**, sin regenerar el perfil desde cero.  \n",
    "4. Usa **TrustCall** para manejar la extracción de datos estructurados de manera eficiente.\n",
    "\n",
    "#### Esquema del Perfil de Usuario\n",
    "```python\n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\" Perfil del usuario \"\"\"\n",
    "    user_name: str = Field(description=\"El nombre preferido del usuario\")\n",
    "    user_location: str = Field(description=\"La ubicación del usuario\")\n",
    "    interests: list = Field(description=\"Lista de intereses del usuario\")\n",
    "```\n",
    "- **Define la estructura de datos** para almacenar los detalles del usuario.  \n",
    "- Incluye:\n",
    "  - **`user_name`**: Nombre preferido del usuario.  \n",
    "  - **`user_location`**: Ubicación del usuario.  \n",
    "  - **`interests`**: Lista de intereses del usuario.  \n",
    "\n",
    "- Cada campo tiene una **descripción** para guiar a la IA (el modelo LLM) al extraer información.\n",
    "\n",
    "#### Crear el Extractor TrustCall\n",
    "```python\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\",\n",
    ")\n",
    "```\n",
    "- Configura **TrustCall** para extraer datos basados en el **esquema `UserProfile`**.  \n",
    "- **`tool_choice`**: Asegura que la IA (el modelo LLM) solo extraiga los campos especificados en este esquema, evitando datos irrelevantes.\n",
    "\n",
    "#### Descripción del Flujo de Trabajo\n",
    "\n",
    "**Paso 1: Responder a los Mensajes (`call_model`)**\n",
    "```python\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "- **Carga la memoria** del usuario desde el almacenamiento.  \n",
    "- Si la memoria existe, formatea los datos para incluir:\n",
    "  - **Nombre**  \n",
    "  - **Ubicación**  \n",
    "  - **Intereses**  \n",
    "- La IA responde al usuario utilizando esta **memoria para personalización**:\n",
    "```python\n",
    "response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "```\n",
    "\n",
    "**Paso 2: Actualizar la Memoria (`write_memory`)**\n",
    "```python\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "- **Extrae nueva información** de la conversación y **actualiza la memoria**.  \n",
    "\n",
    "**Pasos Clave:**\n",
    "1. **Obtener la Memoria Existente:**\n",
    "```python\n",
    "existing_profile = {\"UserProfile\": existing_memory.value} if existing_memory else None\n",
    "```\n",
    "- Recupera el perfil actual si existe.  \n",
    "\n",
    "2. **Invocar el Extractor TrustCall:**\n",
    "```python\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)]+state[\"messages\"], \"existing\": existing_profile})\n",
    "```\n",
    "- Usa TrustCall para:\n",
    "  - Analizar los mensajes de la conversación.  \n",
    "  - Actualizar solo los **campos relevantes** en el perfil, dejando los demás sin cambios.  \n",
    "\n",
    "3. **Guardar el Perfil Actualizado:**\n",
    "```python\n",
    "updated_profile = result[\"responses\"][0].model_dump()\n",
    "store.put(namespace, key, updated_profile)\n",
    "```\n",
    "- Convierte el resultado a JSON y **lo guarda en memoria**.\n",
    "\n",
    "#### Gestión de Memoria\n",
    "\n",
    "**Memoria a Corto Plazo (Memoria de Sesión)**\n",
    "```python\n",
    "within_thread_memory = MemorySaver()\n",
    "```\n",
    "- Mantiene la memoria durante **conversaciones en curso** (temporal).  \n",
    "\n",
    "**Memoria a Largo Plazo (Memoria Persistente)**\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "```\n",
    "- Almacena **perfiles de usuario de manera persistente** para **futuras sesiones**.  \n",
    "\n",
    "#### Graph\n",
    "```python\n",
    "builder = StateGraph(MessagesState)\n",
    "```\n",
    "- Define un **flujo de trabajo basado en estados** para el chatbot:\n",
    "1. **INICIO → call_model**:  \n",
    "   - Responder al usuario utilizando la memoria existente.  \n",
    "2. **call_model → write_memory**:  \n",
    "   - Extraer y actualizar la memoria del usuario según la conversación más reciente.  \n",
    "3. **write_memory → FIN**:  \n",
    "   - Guardar la memoria actualizada y finalizar el flujo de trabajo.\n",
    "\n",
    "**Visualización del Graph**\n",
    "```python\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "```\n",
    "- Genera un **diagrama** para visualizar los pasos del flujo de trabajo del chatbot.\n",
    "\n",
    "#### Escenario de Ejemplo\n",
    "\n",
    "**Conversación de Entrada:**\n",
    "```\n",
    "Humano: Hola, soy Sarah.\n",
    "IA: Encantado de conocerte, Sarah.\n",
    "Humano: Soy de Nueva York y me encanta hacer senderismo y fotografía.\n",
    "```\n",
    "\n",
    "**Actualización de Memoria:**\n",
    "```\n",
    "{\n",
    "    \"user_name\": \"Sarah\",\n",
    "    \"user_location\": \"Nueva York\",\n",
    "    \"interests\": [\"senderismo\", \"fotografía\"]\n",
    "}\n",
    "```\n",
    "\n",
    "**Siguiente Conversación:**\n",
    "```\n",
    "Humano: Dime cuáles son las mejores rutas de senderismo cerca de Nueva York.\n",
    "IA: Dado que te encanta el senderismo y estás en Nueva York, déjame sugerirte algunas rutas excelentes cerca de ti...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d96b4-09d1-43ca-beab-b71e3d47dbe9",
   "metadata": {},
   "source": [
    "## Bien, veamos cómo funciona esta aplicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "332d84b7-7c11-4c6e-aa38-2970dc1f6717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Julio\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Julio! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "746309f6-f18b-4b40-87a2-d4f8f8080c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm interested in Gen AI Startups\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's great, Julio! Generative AI is a rapidly growing field with a lot of exciting developments. Are you looking for information on specific startups, trends in the industry, or perhaps advice on getting involved with a generative AI startup? Let me know how I can help!\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I'm interested in Gen AI Startups\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e7e46a9-d22b-470b-be39-2cb6ebdf09c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'user_name': 'Julio',\n",
       "  'user_location': '',\n",
       "  'interests': ['Gen AI Startups']},\n",
       " 'key': 'user_memory',\n",
       " 'namespace': ['memory', '1'],\n",
       " 'created_at': '2025-01-07T10:04:01.754010+00:00',\n",
       " 'updated_at': '2025-01-07T10:04:01.754012+00:00'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54808e5b-e570-4bf6-a93f-93bee95c3316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Julio', 'user_location': '', 'interests': ['Gen AI Startups']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The user profile saved as a JSON object\n",
    "existing_memory.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf6357ed-1dcc-4150-85d4-a1e6f7f2ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I am also interested in the latest trends about AI Agents\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "AI agents are evolving rapidly, and several key trends are shaping their development and application:\n",
      "\n",
      "1. **Increased Autonomy**: AI agents are becoming more autonomous, capable of performing complex tasks without human intervention. This includes decision-making, problem-solving, and adapting to new environments.\n",
      "\n",
      "2. **Multi-Modal Capabilities**: AI agents are increasingly integrating multiple types of data, such as text, images, and audio, to provide more comprehensive and context-aware interactions.\n",
      "\n",
      "3. **Personalization**: There's a growing focus on creating AI agents that can tailor their interactions and responses based on individual user preferences and behaviors, enhancing user experience.\n",
      "\n",
      "4. **Ethical and Responsible AI**: As AI agents become more prevalent, there's a stronger emphasis on ensuring they operate ethically, with transparency and accountability in their decision-making processes.\n",
      "\n",
      "5. **Collaboration with Humans**: AI agents are being designed to work alongside humans, augmenting human capabilities rather than replacing them. This includes applications in customer service, healthcare, and education.\n",
      "\n",
      "6. **Real-Time Learning and Adaptation**: AI agents are being developed to learn and adapt in real-time, allowing them to improve their performance and relevance over time.\n",
      "\n",
      "7. **Integration with IoT and Smart Devices**: AI agents are increasingly being integrated with Internet of Things (IoT) devices, enabling smarter home and industrial automation solutions.\n",
      "\n",
      "These trends indicate a future where AI agents will play a more significant role in both personal and professional settings, offering enhanced capabilities and efficiencies. If there's a particular trend or application you're curious about, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I am also interested in the latest trends about AI Agents\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e8610-316d-4f02-9bea-5e37a978aaae",
   "metadata": {},
   "source": [
    "## Revisemos lo que acabamos de hacer\n",
    "\n",
    "Este código demuestra cómo el chatbot **recopila, actualiza y recupera las preferencias del usuario** utilizando el **flujo de trabajo basado en graphs** y el **extractor TrustCall** configurado anteriormente. Utiliza tanto **memoria a corto plazo (sesión específica)** como **memoria a largo plazo (persistente)** para **personalizar respuestas** y **actualizar progresivamente el perfil del usuario**. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Configurar la Memoria\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "```\n",
    "- **ID de Hilo (`thread_id`)**: Rastrea la **memoria a corto plazo** dentro de la conversación actual (sesión).  \n",
    "- **ID de Usuario (`user_id`)**: Rastrea la **memoria a largo plazo** a lo largo de múltiples sesiones.  \n",
    "- Asegura que los datos sean **almacenados y recuperados** en función del ID del usuario (\"1\").\n",
    "\n",
    "#### Primera Entrada (Input) del Usuario - Nombre\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"Hola, mi nombre es Julio\")]\n",
    "\n",
    "# Ejecutar el gráfico\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "\n",
    "**¿Qué sucede aquí?**\n",
    "1. **Mensaje de Entrada**: El usuario dice que su nombre es \"Julio\".  \n",
    "2. **Flujo de Trabajo del Graph**:\n",
    "   - **Paso 1:** Responde al usuario usando la memoria existente (actualmente vacía).  \n",
    "   - **Paso 2:** Actualiza la memoria para guardar **\"Julio\"** como **user_name**.  \n",
    "3. **Salida**: La IA (el modelo LLM) responde, por ejemplo: *\"¡Hola Julio! Encantado de conocerte.\"*  \n",
    "\n",
    "#### Segunda Entrada del Usuario - Interés en Startups de IA Generativa\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"Estoy interesado en Startups de IA Generativa\")]\n",
    "\n",
    "# Ejecutar el gráfico\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "\n",
    "**¿Qué sucede aquí?**\n",
    "1. **Mensaje de Entrada**: El usuario menciona su interés en **Startups de IA Generativa**.  \n",
    "2. **Flujo de Trabajo del Graph**:\n",
    "   - **Paso 1:** Responde en función de la memoria existente (user_name = Julio).  \n",
    "   - **Paso 2:** Actualiza la memoria, agregando **\"Startups de IA Generativa\"** a la lista de **intereses**.  \n",
    "3. **Salida**: La IA responde, por ejemplo: *\"¡Eso es genial, Julio! Las startups de IA generativa son fascinantes.\"*  \n",
    "\n",
    "#### Recuperar y Examinar la Memoria\n",
    "```python\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()\n",
    "existing_memory.value\n",
    "```\n",
    "\n",
    "**¿Qué sucede aquí?**\n",
    "1. **Recuperar Memoria**:\n",
    "   - Obtiene el perfil guardado del usuario (ID = 1) desde la **memoria a largo plazo**.  \n",
    "2. **Contenido de la Memoria**:\n",
    "   - Muestra el perfil como un **objeto JSON**:\n",
    "   ```json\n",
    "   {\n",
    "       \"user_name\": \"Julio\",\n",
    "       \"user_location\": null,\n",
    "       \"interests\": [\"Startups de IA Generativa\"]\n",
    "   }\n",
    "   ```\n",
    "3. **Observaciones Clave**:\n",
    "   - **Actualizaciones Incrementales**: La memoria solo agregó **nueva información** (interés en Startups de IA Generativa) sin sobrescribir datos existentes (user_name = Julio).  \n",
    "   - **Preserva Campos No Proporcionados**: Mantiene campos como **ubicación** en **null** si no se ha proporcionado información.\n",
    "\n",
    "#### Tercera Entrada (Input) del Usuario - Interés Adicional\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"También me interesan las últimas tendencias sobre Agentes de IA\")]\n",
    "\n",
    "# Ejecutar el gráfico\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "\n",
    "**¿Qué sucede aquí?**\n",
    "1. **Mensaje de Entrada**: El usuario agrega **Agentes de IA** como otro interés.  \n",
    "2. **Flujo de Trabajo del Gráfico**:\n",
    "   - **Paso 1:** Responde usando la memoria actualizada (user_name = Julio, interests = [\"Startups de IA Generativa\"]).  \n",
    "   - **Paso 2:** Actualiza la memoria, **agregando** **\"Agentes de IA\"** a la lista de **intereses**.  \n",
    "3. **Salida**: La IA responde, por ejemplo: *\"Los Agentes de IA son un tema candente, Julio. ¡Gran adición a tus intereses!\"*  \n",
    "\n",
    "#### Memoria Actualizada Después de la Tercera Entrada (Input)\n",
    "```json\n",
    "{\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"user_location\": null,\n",
    "    \"interests\": [\"Startups de IA Generativa\", \"Agentes de IA\"]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Reflexiones Finales\n",
    "\n",
    "Este código demuestra cómo el chatbot:\n",
    "1. **Recuerda las preferencias del usuario** a lo largo de las sesiones mediante memoria persistente.  \n",
    "2. **Agrega nueva información de manera incremental** sin sobrescribir datos anteriores.  \n",
    "3. **Extrae y organiza los detalles del usuario** usando **TrustCall** en un **schema estructurado**.  \n",
    "4. **Personaliza las interacciones**, haciendo las conversaciones más naturales y amigables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d853e2-a4a2-4d47-aa80-04e1eab9f8b8",
   "metadata": {},
   "source": [
    "## ¡Se ve bien! Ahora que sabemos que nuestro chatbot tiene esta memoria a largo plazo más estructurada, podemos continuar nuestra conversación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a57cab59-1f79-42e0-9e99-92525904d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What online magazines do you recommend for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Given your interests in Gen AI Startups and AI Agent Startups, I recommend the following online magazines and publications:\n",
      "\n",
      "1. **TechCrunch** - They frequently cover the latest in technology startups, including AI and machine learning innovations.\n",
      "\n",
      "2. **VentureBeat** - Known for its focus on transformative technology, VentureBeat often features articles on AI advancements and startup news.\n",
      "\n",
      "3. **Wired** - While it covers a broad range of tech topics, Wired often delves into AI and the impact of technology on society.\n",
      "\n",
      "4. **MIT Technology Review** - Offers in-depth articles on emerging technologies, including AI and its applications in various industries.\n",
      "\n",
      "5. **AI Trends** - Specifically focused on AI, this publication provides insights into the latest trends and developments in the AI space.\n",
      "\n",
      "6. **The Verge** - Covers the intersection of technology, science, art, and culture, with frequent articles on AI and tech startups.\n",
      "\n",
      "These publications should provide you with a wealth of information and insights into the areas you're interested in.\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"What online magazines do you recommend for me?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06947f75-c8ee-4c9e-9400-15daa810a187",
   "metadata": {},
   "source": [
    "## Cómo ejecutar el código desde Visual Studio Code\n",
    "* En Visual Studio Code, busca el archivo `026-profile-schema.py`.\n",
    "* En la terminal, asegúrate de estar en el directorio del archivo y ejecuta:\n",
    "    * `python 026-profile-schema.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe21ec3-f917-46c1-a02f-3adb330a8d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
