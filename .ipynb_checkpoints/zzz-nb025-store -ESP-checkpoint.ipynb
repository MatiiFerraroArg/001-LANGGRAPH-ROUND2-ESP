{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d0ff32-dde7-4dbc-a315-f34e308dbc5a",
   "metadata": {},
   "source": [
    "# Store  \n",
    "* Presentaremos el **Memory Store de LangGraph** como una forma de **guardar y recuperar memorias a largo plazo**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60feb5d9-128c-4fe0-8b1e-d547226bc124",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e336-d054-412c-8a3f-1fbec63e1bcd",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda8d4-80cf-4b8f-9981-94edda5e9911",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 025-store.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e766aa-f3e2-491f-be99-d0c6b700d47a",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df978ec5-bfd2-4167-bd33-86bc2687d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel35 = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chatModel4o = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb2929-16bf-4626-aae1-91e60fcae78a",
   "metadata": {},
   "source": [
    "## Construyamos un chatbot que pueda recordar datos sobre el usuario  \n",
    "* Crearemos un chatbot que utilice **tanto memoria a corto plazo (dentro del hilo, del thread) como memoria a largo plazo (entre hilos, entre threads)**.  \n",
    "* Nos enfocaremos en la **memoria semántica a largo plazo (long-term semantic memory)**, que permite recordar datos sobre el usuario. Estas memorias a largo plazo se utilizarán para crear un chatbot personalizado que pueda recordar información relevante sobre el usuario.  \n",
    "* La aplicación **guardará la memoria \"en la ruta rápida\" (save memory \"in the hot path\")**, mientras el usuario conversa con ella."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38085689-302c-4afb-a6d3-dd063e8eabd2",
   "metadata": {},
   "source": [
    "## Utilizaremos el Memory Store de LangGraph (también llamado simplemente Store) para construir la memoria a largo plazo de nuestro chatbot  \n",
    "\n",
    "Memoria a largo plazo:  \n",
    "* **Alcance:** Funciona **a través de sesiones e hilos (threads)**.  \n",
    "* **Caso de uso:** Permite recordar **información sobre el usuario entre sesiones**.  \n",
    "* **El Memory Store de LangGraph** proporciona un método para **almacenar y recuperar información** entre hilos (threads) en LangGraph.  \n",
    "* Utiliza una **clase de código abierto (open source class)** ([enlace aquí](https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/)) para manejar **persistent `key-value` stores**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267d4e63-4f41-4ad0-a839-7b05c58f1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "in_memory_store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f828bebe-9aa0-4614-9101-880f7ca7a15d",
   "metadata": {},
   "source": [
    "Al almacenar objetos en el **Store**, proporcionamos:  \n",
    "* **El namespace** (para entendernos, es algo así como cuando utilizamos el nombre de un directorio en nuestro trabajo como ingenieros de software).  \n",
    "* **La clave (key)** (para entendernos, es algo así como cuando utilizamos el nombre de un archivo).  \n",
    "* **El valor (value)** (para entendernos, es algo así como cuando utilizamos el contenido de un archivo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4222b97-c66b-4649-81a1-dab2ef05a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "key = str(uuid.uuid4())\n",
    "\n",
    "# The value needs to be a dictionary  \n",
    "value = {\"food_preference\" : \"I like pizza\"}\n",
    "\n",
    "# Save\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5bc930-3af3-4003-8652-5f8fb2267fd0",
   "metadata": {},
   "source": [
    "## Expliquemos el código anterior en términos sencillos  \n",
    "\n",
    "Este código muestra cómo almacenar datos en memoria utilizando **InMemoryStore de LangGraph**. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Importar los módulos necesarios  \n",
    "```python\n",
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "```\n",
    "- **uuid**: Genera identificadores únicos (útil para crear keys únicas).  \n",
    "- **InMemoryStore**: Una herramienta de LangGraph para almacenar pares key-value en memoria.  \n",
    "\n",
    "#### Crear un objeto de almacenamiento en memoria  \n",
    "```python\n",
    "in_memory_store = InMemoryStore()\n",
    "```\n",
    "- Crea una instancia de **InMemoryStore** para almacenar datos temporalmente (no se guarda en un archivo ni en una base de datos).  \n",
    "\n",
    "#### Definir identificadores  \n",
    "```python\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "```\n",
    "- **user_id**: Representa el ID del usuario (en este caso, \"1\").  \n",
    "- **namespace_for_memory**: Una tupla (tuple, formato de python) que combina el ID del usuario y la categoría (\"memories\").  \n",
    "  - Esto ayuda a **organizar los datos** para cada usuario y contexto.  \n",
    "\n",
    "#### Generar una clave única (unique key) \n",
    "```python\n",
    "key = str(uuid.uuid4())\n",
    "```\n",
    "- **uuid.uuid4()**: Crea un identificador único aleatorio.  \n",
    "- **str()**: Lo convierte en un string para poder usarlo como clave.  \n",
    "\n",
    "#### Preparar los datos a almacenar  \n",
    "```python\n",
    "value = {\"food_preference\" : \"Me gusta la pizza\"}\n",
    "```\n",
    "- **value**: Un diccionario que almacena información (en este caso, la preferencia de comida del usuario).  \n",
    "\n",
    "#### Almacenar los datos  \n",
    "```python\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "```\n",
    "- Guarda el diccionario (**value**) en memoria bajo el **namespace_for_memory** y la **clave (key) generada**.  \n",
    "- Esto significa que los datos se almacenan en un \"directorio\" (namespace) específico para el usuario y pueden recuperarse más tarde."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e2aab-621b-46c6-bb44-c4213ecee25c",
   "metadata": {},
   "source": [
    "## Cómo recuperar memorias de la Memory Store  \n",
    "* **Usamos `search`** para recuperar objetos del store utilizando el **namespace**.  \n",
    "* Esto devuelve una **lista** con los elementos almacenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e72a12-54a3-4739-ab3e-ea022f551838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'food_preference': 'I like pizza'},\n",
       " 'key': '4803d5a1-5c53-4f91-969e-b507b5b197c5',\n",
       " 'namespace': ['1', 'memories'],\n",
       " 'created_at': '2025-01-30T15:49:26.755675+00:00',\n",
       " 'updated_at': '2025-01-30T15:49:26.755677+00:00',\n",
       " 'score': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "\n",
    "memories[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2efc8d29-902c-4eab-8a7b-a97e365fcc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4803d5a1-5c53-4f91-969e-b507b5b197c5', {'food_preference': 'I like pizza'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories[0].key, memories[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a492496a-6c2d-4a2b-8826-058a421db794",
   "metadata": {},
   "source": [
    "* **También podemos usar `get`** para recuperar un objeto utilizando el **namespace** y la **clave (key)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a862c23-d41e-4e73-95f2-954a21d7272a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'food_preference': 'I like pizza'},\n",
       " 'key': '4803d5a1-5c53-4f91-969e-b507b5b197c5',\n",
       " 'namespace': ['1', 'memories'],\n",
       " 'created_at': '2025-01-30T15:49:26.755675+00:00',\n",
       " 'updated_at': '2025-01-30T15:49:26.755677+00:00'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = in_memory_store.get(namespace_for_memory, key)\n",
    "memory.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8da64e68-52a7-4ded-b2d3-577e6349d9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4803d5a1-5c53-4f91-969e-b507b5b197c5', {'food_preference': 'I like pizza'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.key, memory.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2c83e-947e-46bf-8dc3-c42e1132450d",
   "metadata": {},
   "source": [
    "## OK. Ahora que conocemos mejor la Memory Store, comencemos a construir el chatbot con memoria a largo plazo  \n",
    "\n",
    "Queremos un chatbot que tenga **dos tipos de memoria**:  \n",
    "\n",
    "1. **`Memoria a corto plazo (dentro del hilo, within-thread)`**:  \n",
    "   - El chatbot puede **persistir el historial de la conversación** y/o permitir **interrupciones** en una sesión de chat.  \n",
    "\n",
    "2. **`Memoria a largo plazo (entre hilos, cross-thread)`**:  \n",
    "   - El chatbot puede **recordar información sobre un usuario específico** *a lo largo de todas las sesiones de chat*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441998e-3742-4f5c-a224-a8493c899a24",
   "metadata": {},
   "source": [
    "* **Para la memoria a corto plazo usaremos un *checkpointer***.  \n",
    "* Y **para la memoria a largo plazo usaremos la *Memory Store***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d732d092-df52-4c34-a452-4adda05e0c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat model \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb286232-2fd5-4123-830b-9b3b5b2e0369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"\"You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, location)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "\n",
    "Based on the chat history below, please update the user information:\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    # Extract the actual memory content if it exists and add a prefix\n",
    "    if existing_memory:\n",
    "        # Value is a dictionary with a memory key\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "    \n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve existing memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "        \n",
    "    # Extract the memory\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
    "    new_memory = model.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "\n",
    "    # Overwrite the existing memory in the store \n",
    "    key = \"user_memory\"\n",
    "\n",
    "    # Write value as a dictionary with a memory key\n",
    "    store.put(namespace, key, {\"memory\": new_memory.content})\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d9355-6809-41f4-86dd-e48ea7d487ec",
   "metadata": {},
   "source": [
    "## Expliquemos en términos sencillos la primera función relevante del código anterior: `call_model`  \n",
    "\n",
    "La función **`call_model`** genera una **respuesta personalizada del chatbot** utilizando la memoria almacenada sobre el usuario. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "\n",
    "#### Propósito de la función \n",
    "La función:  \n",
    "- **Carga la memoria** del usuario desde el almacenamiento.  \n",
    "- **Personaliza** la respuesta del chatbot con la información guardada y los mensajes actuales de la conversación.\n",
    "\n",
    "#### Parámetros de Entrada  \n",
    "```python\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "- **`state`**: Contiene los mensajes actuales del chat.  \n",
    "- **`config`**: Proporciona detalles de configuración, como el ID del usuario.  \n",
    "- **`store`**: Maneja el sistema de almacenamiento de memoria (donde se guardan los datos del usuario).  \n",
    "\n",
    "#### Obtener el ID del Usuario  \n",
    "```python\n",
    "user_id = config[\"configurable\"][\"user_id\"]\n",
    "```\n",
    "- Extrae el **ID del usuario** desde la configuración.  \n",
    "- Este ID se usa para buscar la memoria correspondiente a ese usuario en el almacenamiento.\n",
    "\n",
    "#### Recuperar la Memoria del Usuario desde el Almacenamiento  \n",
    "```python\n",
    "namespace = (\"memory\", user_id)\n",
    "key = \"user_memory\"\n",
    "existing_memory = store.get(namespace, key)\n",
    "```\n",
    "- **`namespace`** organiza la memoria dentro de un grupo llamado `\"memory\"` y el ID del usuario.  \n",
    "- **`key`** define qué información recuperar (por ejemplo, `\"user_memory\"`).  \n",
    "- **`store.get()`** busca los datos de memoria para ese usuario.\n",
    "\n",
    "#### Verificar si Existe Memoria Almacenada  \n",
    "```python\n",
    "if existing_memory:\n",
    "    existing_memory_content = existing_memory.value.get('memory')\n",
    "else:\n",
    "    existing_memory_content = \"No existing memory found.\"\n",
    "```\n",
    "- Si **hay memoria almacenada**, la extrae del almacenamiento.  \n",
    "- Si **no hay memoria**, devuelve un mensaje indicando que no hay datos previos.\n",
    "\n",
    "#### Crear un Mensaje del Sistema con la Memoria  \n",
    "```python\n",
    "system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "```\n",
    "- Genera un **mensaje del sistema** (una instrucción para el chatbot) con el contenido de la memoria.  \n",
    "- **Ejemplo:** *\"Eres un asistente útil. El usuario ama la pizza.\"*\n",
    "\n",
    "#### Generar la Respuesta del Chatbot  \n",
    "```python\n",
    "response = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "```\n",
    "- **Combina** el mensaje del sistema (con la memoria) y el historial del chat del usuario.  \n",
    "- Usa el **modelo GPT-4o** para generar una **respuesta personalizada**.\n",
    "\n",
    "#### Devolver la Respuesta  \n",
    "```python\n",
    "return {\"messages\": response}\n",
    "```\n",
    "- **Devuelve la respuesta generada** en un diccionario para que el chatbot la use.\n",
    "\n",
    "#### Ejemplo de Funcionamiento  \n",
    "\n",
    "**Memoria Almacenada:**  \n",
    "- **ID del usuario:** 123  \n",
    "- **Memoria:** *\"Nombre: Juan. Le gusta el senderismo.\"*  \n",
    "\n",
    "**Mensaje Nuevo en el Chat:**  \n",
    "> \"Estoy planeando un viaje el próximo fin de semana.\"\n",
    "\n",
    "**Proceso:**  \n",
    "1. Recupera memoria: *\"Nombre: Juan. Le gusta el senderismo.\"*  \n",
    "2. Genera el mensaje del sistema: *\"Eres un asistente útil. El usuario ama el senderismo.\"*  \n",
    "3. Usa esta información para generar una respuesta personalizada.  \n",
    "\n",
    "**Salida del Chatbot:**  \n",
    "> \"¡Eso suena emocionante, Juan! ¿Estás planeando hacer senderismo en algún lugar especial?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a2759-56d1-4c5e-91b1-df48be1188b7",
   "metadata": {},
   "source": [
    "## OK. Ahora expliquemos en términos sencillos la otra función relevante en el código anterior: `write_memory`  \n",
    "\n",
    "La función **`write_memory`** **actualiza y guarda la memoria del usuario** basándose en el historial del chat. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Propósito de la función  \n",
    "La función:  \n",
    "- **Analiza el historial del chat** para identificar nueva información sobre el usuario.  \n",
    "- **Actualiza o sobrescribe la memoria** con la nueva información.  \n",
    "- **Guarda la memoria actualizada** para futuras conversaciones.  \n",
    "\n",
    "#### Parámetros de Entrada  \n",
    "```python\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "- **`state`**: Contiene los mensajes actuales del chat.  \n",
    "- **`config`**: Proporciona configuraciones como el ID del usuario.  \n",
    "- **`store`**: Administra dónde se almacena la memoria.  \n",
    "\n",
    "#### Obtener el ID del Usuario  \n",
    "```python\n",
    "user_id = config[\"configurable\"][\"user_id\"]\n",
    "```\n",
    "- Recupera el **ID del usuario** desde la configuración.  \n",
    "- Este ID se usa para identificar la memoria específica del usuario en el almacenamiento.  \n",
    "\n",
    "#### Recuperar la Memoria Existente  \n",
    "```python\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = store.get(namespace, \"user_memory\")\n",
    "```\n",
    "- Define un **namespace** (estructura similar a una carpeta) basado en el ID del usuario.  \n",
    "- Intenta **recuperar la memoria** etiquetada como **\"user_memory\"** desde el almacenamiento.  \n",
    "\n",
    "#### Verificar si Existe Memoria  \n",
    "```python\n",
    "if existing_memory:\n",
    "    existing_memory_content = existing_memory.value.get('memory')\n",
    "else:\n",
    "    existing_memory_content = \"No existing memory found.\"\n",
    "```\n",
    "- Si **existe memoria**, extrae su contenido.  \n",
    "- Si **no hay memoria previa**, asume que no hay información guardada.  \n",
    "\n",
    "#### Analizar el Chat y Generar la Memoria Actualizada  \n",
    "```python\n",
    "system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
    "new_memory = model.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "```\n",
    "- **Crea instrucciones** (de `CREATE_MEMORY_INSTRUCTION`) para que el chatbot **extraiga nueva información** sobre el usuario.  \n",
    "- Combina:\n",
    "  - La memoria existente.  \n",
    "  - El historial de mensajes recientes del chat.  \n",
    "- Usa el **modelo GPT-4o** para **analizar la conversación y generar la memoria actualizada**.  \n",
    "\n",
    "#### Guardar la Memoria Actualizada  \n",
    "```python\n",
    "key = \"user_memory\"\n",
    "store.put(namespace, key, {\"memory\": new_memory.content})\n",
    "```\n",
    "- **Guarda la nueva memoria** en el almacenamiento, reemplazando la antigua.  \n",
    "- La memoria se almacena como un **diccionario** con la clave **\"memory\"** para facilitar su recuperación.  \n",
    "\n",
    "#### Ejemplo de Funcionamiento  \n",
    "\n",
    "**Memoria Inicial:**  \n",
    "- Guardada: *\"Nombre: Juan. Le gusta el senderismo.\"*  \n",
    "\n",
    "**Mensaje Nuevo en el Chat:**  \n",
    "> \"También disfruto andar en bicicleta y recientemente visité España.\"  \n",
    "\n",
    "**Proceso:**  \n",
    "1. Recupera memoria: *\"Nombre: Juan. Le gusta el senderismo.\"*  \n",
    "2. Analiza el historial del chat: Agrega *\"Le gusta andar en bicicleta, visitó España.\"*  \n",
    "3. Memoria actualizada:  \n",
    "   ```\n",
    "   - Nombre: Juan  \n",
    "   - Le gusta el senderismo  \n",
    "   - Disfruta andar en bicicleta  \n",
    "   - Visitó España\n",
    "   ```\n",
    "4. Guarda esta nueva memoria.  \n",
    "\n",
    "#### Resultado (Nueva Memoria Guardada):  \n",
    "- En futuras conversaciones, el chatbot **recordará que Juan también disfruta andar en bicicleta y que ha estado en España**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b4e7e0-4756-4fab-b13f-33e9fb4daf20",
   "metadata": {},
   "source": [
    "## OK. Ahora expliquemos en términos sencillos todo el código anterior (incluyendo las dos funciones)  \n",
    "\n",
    "Este código define un **chatbot con memoria**. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Importar las herramientas necesarias  \n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "```\n",
    "- **ChatOpenAI** inicializa el modelo **GPT-4o** para procesar las entradas del usuario y generar respuestas.  \n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "```\n",
    "- **StateGraph**: Permite definir el flujo de trabajo del chatbot usando nodos y conexiones.  \n",
    "- **MemorySaver** y **BaseStore**: Manejan la memoria de corto y largo plazo.  \n",
    "\n",
    "#### Configurar el modelo de chat  \n",
    "```python\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "```\n",
    "- **model**: Usa GPT-4o con **cero aleatoriedad** (**temperature=0**) para respuestas consistentes.  \n",
    "\n",
    "#### Definir las instrucciones del chatbot  \n",
    "```python\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"Eres un asistente útil con memoria que proporciona información sobre el usuario. \n",
    "Si tienes memoria de este usuario, úsala para personalizar tus respuestas.\n",
    "Aquí está la memoria (puede estar vacía): {memory}\"\"\"\n",
    "```\n",
    "- Define las **instrucciones del sistema** para el chatbot.  \n",
    "- Incluye **memoria** sobre el usuario para que las respuestas sean **personalizadas**.  \n",
    "\n",
    "```python\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"Estás recopilando información sobre el usuario para personalizar tus respuestas.\n",
    "\n",
    "INFORMACIÓN ACTUAL DEL USUARIO:\n",
    "{memory}\n",
    "\n",
    "INSTRUCCIONES:\n",
    "1. Revisa detenidamente el historial del chat.\n",
    "2. Identifica nueva información sobre el usuario, como:\n",
    "   - Datos personales (nombre, ubicación).\n",
    "   - Preferencias (gustos, disgustos).\n",
    "   - Intereses y pasatiempos.\n",
    "   - Experiencias pasadas.\n",
    "   - Objetivos o planes futuros.\n",
    "3. Combina la nueva información con la memoria existente.\n",
    "4. Formatea la memoria en una lista con viñetas.\n",
    "5. Si hay información contradictoria, conserva la versión más reciente.\n",
    "\n",
    "Recuerda: Solo incluye información que el usuario haya mencionado directamente. No hagas suposiciones.\n",
    "\n",
    "Con base en el historial del chat, por favor actualiza la información del usuario:\"\"\"\n",
    "```\n",
    "- Especifica **cómo actualizar la memoria** según el historial del chat (preferencias, objetivos, datos personales).  \n",
    "- Se asegura de que las actualizaciones sean **precisas y organizadas** en una lista con viñetas.  \n",
    "\n",
    "#### Procesar los mensajes y la memoria  \n",
    "\n",
    "**Paso 1: Recuperar la memoria y generar la respuesta**  \n",
    "```python\n",
    "def call_model(state, config, store):\n",
    "```\n",
    "- **Propósito**: Recupera la memoria existente y la usa para **personalizar la respuesta** del chatbot.  \n",
    "\n",
    "**Pasos clave:**  \n",
    "1. **Obtiene el ID del usuario** desde la configuración.  \n",
    "2. **Recupera la memoria** almacenada del usuario.  \n",
    "3. **Genera una respuesta** combinando la memoria con el historial del chat.  \n",
    "4. **Devuelve la respuesta actualizada**.  \n",
    "\n",
    "**Paso 2: Actualizar la memoria**  \n",
    "```python\n",
    "def write_memory(state, config, store):\n",
    "```\n",
    "- **Propósito**: **Analiza el historial del chat y guarda nueva información** sobre el usuario.  \n",
    "\n",
    "**Pasos clave:**  \n",
    "1. **Recupera la memoria existente** (si la hay).  \n",
    "2. **Genera un mensaje de sistema** para que el modelo **extraiga nueva información** del chat.  \n",
    "3. **Guarda la memoria actualizada** en el almacenamiento, reemplazando la anterior.  \n",
    "\n",
    "#### Construcción del graph  \n",
    "```python\n",
    "builder = StateGraph(MessagesState)\n",
    "```\n",
    "- **StateGraph** define el proceso del chatbot como una serie de pasos.  \n",
    "\n",
    "**Nodos (pasos):**  \n",
    "1. **`call_model`**: Genera una respuesta usando la memoria.  \n",
    "2. **`write_memory`**: Actualiza la memoria después de responder.  \n",
    "\n",
    "**Edges (flujo):**  \n",
    "- El flujo comienza en **START**, luego:  \n",
    "  - Pasa por **call_model**, donde se genera la respuesta.  \n",
    "  - Luego a **write_memory**, donde se actualiza la memoria.  \n",
    "  - Finaliza en **END**.  \n",
    "\n",
    "#### Gestión de la memoria  \n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "```\n",
    "- **InMemoryStore**: Guarda la memoria entre sesiones (almacenamiento a largo plazo).  \n",
    "\n",
    "```python\n",
    "within_thread_memory = MemorySaver()\n",
    "```\n",
    "- **MemorySaver**: Maneja la memoria dentro de una sola sesión (almacenamiento a corto plazo).  \n",
    "\n",
    "#### Compilar y visualizar el flujo de trabajo  \n",
    "```python\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "```\n",
    "- Combina los pasos en un **pipeline funcional** para el chatbot con memoria.  \n",
    "\n",
    "```python\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "```\n",
    "- **Muestra el flujo de trabajo** como un diagrama visual de nodos y conexiones.  \n",
    "\n",
    "#### ¿Qué hace este chatbot?  \n",
    "1. **Recuerda información del usuario** como preferencias, intereses u objetivos entre sesiones.  \n",
    "2. **Personaliza respuestas** basadas en la memoria almacenada.  \n",
    "3. **Actualiza dinámicamente la memoria** después de cada conversación.  \n",
    "\n",
    "#### Ejemplo de Funcionamiento  \n",
    "\n",
    "**Interacción 1**  \n",
    "**Input:**  \n",
    "> \"Hola, soy Juan y me encanta el senderismo.\"  \n",
    "\n",
    "**Output del chatbot:**  \n",
    "> \"¡Encantado de conocerte, Juan! El senderismo es un gran pasatiempo. ¿Tienes un sendero favorito?\"  \n",
    "\n",
    "**Memoria guardada:**  \n",
    "- **Nombre:** Juan  \n",
    "- **Gusto:** Senderismo  \n",
    "\n",
    "**Interacción 2 (en otra sesión)**  \n",
    "**Input:**  \n",
    "> \"Estoy planeando un viaje a la montaña.\"  \n",
    "\n",
    "**Output del chatbot (utilizando la memoria):**  \n",
    "> \"¡Eso suena perfecto para alguien que ama el senderismo, Juan! ¿Qué montaña planeas visitar?\"  \n",
    "\n",
    "\n",
    "Este ejemplo muestra cómo el chatbot **usa la memoria para mejorar la conversación** y **personalizar sus respuestas** basándose en interacciones previas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba35ef-4e24-44f4-9350-a3d7f70ce805",
   "metadata": {},
   "source": [
    "## Cuando interactuamos con el chatbot, proporcionamos dos elementos:  \n",
    "\n",
    "1. **`Memoria a corto plazo (dentro del hilo, within-thread)`**:  \n",
    "   - Se usa un **`thread ID`** para **persistir el historial del chat** dentro de una sesión.  \n",
    "\n",
    "2. **`Memoria a largo plazo (entre hilos, cross-thread)`**:  \n",
    "   - Se usa un **`user ID`** para **asociar la memoria a largo plazo** con un usuario específico.  \n",
    "\n",
    "Veamos cómo funcionan juntos en la práctica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b83227a-718c-436c-afb5-b1ae9fa02c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Julio\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Julio! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c918dbca-20e5-475f-ae41-bf23999d3413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to drive a vespa around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a lot of fun, Julio! San Francisco is such a beautiful city, and riding a Vespa must be a great way to explore it. Do you have any favorite spots you like to visit while riding around?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to drive a vespa around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46bc50-079e-4511-bba6-f94e9b44bdfb",
   "metadata": {},
   "source": [
    "* We're using the `MemorySaver` checkpointer for within-thread memory.\n",
    "* This saves the chat history to the thread.\n",
    "* We can look at the chat history saved to the thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce41ce3d-ef55-46e3-87ee-f43c76100a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Julio\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Julio! It's nice to meet you. How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to drive a vespa around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a lot of fun, Julio! San Francisco is such a beautiful city, and riding a Vespa must be a great way to explore it. Do you have any favorite spots you like to visit while riding around?\n"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state = graph.get_state(thread).values\n",
    "for m in state[\"messages\"]: \n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e251e11-174a-4ced-b6f9-678fe2714c62",
   "metadata": {},
   "source": [
    "## Expliquemos en términos sencillos lo que acabamos de hacer  \n",
    "\n",
    "El código anterior demuestra cómo funciona el **chatbot con memoria**, procesando las entradas del usuario y actualizando tanto la **memoria a corto plazo** como la **memoria a largo plazo**. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Configurar la Memoria  \n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "```\n",
    "- **`thread_id`**: Se usa para la **memoria a corto plazo** dentro de una sola sesión de chat.  \n",
    "- **`user_id`**: Se usa para la **memoria a largo plazo** y permite recordar información entre sesiones.  \n",
    "- Ambos valores están en **\"1\"** para identificar al usuario y el hilo de conversación.\n",
    "\n",
    "#### Procesar el Primer Mensaje del Usuario  \n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio\")]\n",
    "```\n",
    "- **Entrada del usuario:** \"Hola, mi nombre es Julio.\"  \n",
    "- Este es el **primer mensaje** que se envía al chatbot.\n",
    "\n",
    "#### Ejecutar el Graph\n",
    "```python\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "- **Procesa la entrada** a través del flujo de trabajo del chatbot:  \n",
    "  1. **Recupera la memoria existente** (si la hay).  \n",
    "  2. **Personaliza la respuesta** con la información almacenada.  \n",
    "  3. **Actualiza la memoria** para incluir la nueva información (*El usuario se llama Julio*).  \n",
    "- **Muestra la respuesta del chatbot** en pantalla.\n",
    "\n",
    "#### Procesar el Segundo Mensaje del Usuario  \n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"I like to drive a vespa around San Francisco\")]\n",
    "```\n",
    "- **Entrada del usuario:** \"Me gusta conducir una moto Vespa por San Francisco.\"  \n",
    "- Este es el **segundo mensaje** enviado al chatbot.\n",
    "\n",
    "#### Ejecutar el Graph\n",
    "```python\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "- **Procesa la segunda entrada** de la misma manera:  \n",
    "  1. **Recupera la memoria actualizada** (ya sabe que el usuario se llama Julio).  \n",
    "  2. **Personaliza la respuesta** teniendo en cuenta la memoria existente.  \n",
    "  3. **Guarda la nueva información** (*A Julio le gusta conducir una moto Vespa en San Francisco*).  \n",
    "- **Muestra la respuesta del chatbot** considerando la nueva memoria.\n",
    "\n",
    "#### Revisar la Memoria Guardada (Corto Plazo)  \n",
    "```python\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state = graph.get_state(thread).values\n",
    "for m in state[\"messages\"]: \n",
    "    m.pretty_print()\n",
    "```\n",
    "- **Accede al estado actual de la sesión** usando el `thread_id`.  \n",
    "- **Recupera todos los mensajes** intercambiados hasta el momento (historial del chat).  \n",
    "- **Muestra la conversación completa** para revisión.\n",
    "\n",
    "#### ¿Qué sucede internamente?\n",
    "\n",
    "1. **Primer mensaje (\"Mi nombre es Julio\")**  \n",
    "   - No hay memoria previa, así que se almacena: *\"Nombre: Julio.\"*  \n",
    "   - El chatbot responde reconociendo el nombre:  \n",
    "     > \"¡Hola Julio! ¿En qué puedo ayudarte hoy?\"  \n",
    "\n",
    "2. **Segundo mensaje (\"Me gusta conducir una moto Vespa en San Francisco\")**  \n",
    "   - La memoria ya incluye *\"Nombre: Julio.\"*  \n",
    "   - Se agrega nueva información: *\"A Julio le gusta conducir una moto Vespa en San Francisco.\"*  \n",
    "   - La respuesta del chatbot puede ser:  \n",
    "     > \"¡Eso suena como una forma divertida de explorar San Francisco, Julio!\"  \n",
    "\n",
    "3. **Revisión final:**  \n",
    "   - Se imprimen todos los mensajes del chat y se muestra la memoria almacenada.  \n",
    "\n",
    "#### Ejemplo de Memoria Final Guardada  \n",
    "```\n",
    "- Nombre: Julio  \n",
    "- Le gusta conducir una moto Vespa  \n",
    "- Ubicación: San Francisco  \n",
    "```\n",
    "El chatbot ahora puede **usar esta memoria** en futuras conversaciones sin necesidad de repetir detalles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef750a21-b4af-44a3-84ef-8ed1d7e9365b",
   "metadata": {},
   "source": [
    "## Ahora podemos verificar si la memoria se guardó en el store  \n",
    "* Recordemos que compilamos el graph con el store:  \n",
    "\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "```\n",
    "\n",
    "* Además, agregamos un nodo al graph (`write_memory`) que analiza el historial del chat y **guarda la memoria en el store**.  \n",
    "* Veamos si la memoria realmente se guardó en el **Memory Store**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9da70a3f-6411-4f0b-a9e4-aa845ae6418f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'memory': \"**Updated User Information:**\\n- User's name is Julio.\\n- Likes to drive a Vespa.\\n- Rides around San Francisco.\"},\n",
       " 'key': 'user_memory',\n",
       " 'namespace': ['memory', '1'],\n",
       " 'created_at': '2025-01-30T15:49:39.048129+00:00',\n",
       " 'updated_at': '2025-01-30T15:49:39.048133+00:00'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54c4b9-25c7-43dd-9bbe-759a66eafde4",
   "metadata": {},
   "source": [
    "## Ahora, usemos un `thread_id` diferente para simular una nueva conversación con el mismo usuario  \n",
    "* Usaremos un **nuevo hilo (`thread_id`)** con el **mismo `user_id`**.  \n",
    "* De esta manera, verificaremos que el chatbot **recuerda el perfil del usuario** y lo usa para **personalizar la respuesta** en la nueva conversación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbfd7371-6a3f-4687-adbd-52d8e1044fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi! Where would you recommend I drive with my vehicle in my town?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Julio! Since you enjoy riding your Vespa around San Francisco, there are some fantastic routes you might love. Consider taking a ride through the scenic views of the Golden Gate Bridge, or perhaps a leisurely drive through Golden Gate Park. The winding roads of Lombard Street can be a fun challenge, and the Embarcadero offers beautiful waterfront views. If you're up for a bit of a climb, Twin Peaks provides a stunning panoramic view of the city. Enjoy your ride!\n"
     ]
    }
   ],
   "source": [
    "# We supply a user ID for across-thread memory as well as a new thread ID\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi! Where would you recommend I drive with my vehicle in my town?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300abb86-56ee-4557-871d-46fad7f19d80",
   "metadata": {},
   "source": [
    "* ¡Excelente! Como puedes ver, **el chatbot recuerda mi vehículo y mi ciudad de la conversación anterior**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580064e5-870f-449d-9c71-14b2847d1a8e",
   "metadata": {},
   "source": [
    "## Revisemos lo que acabamos de hacer  \n",
    "\n",
    "El código anterior demuestra cómo el **chatbot con memoria** maneja un **nuevo hilo (thread) de conversación** mientras sigue utilizando la **misma memoria del usuario**. Aquí tienes una explicación paso a paso:\n",
    "\n",
    "#### Configurar una Nueva Conversación  \n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "```\n",
    "- **`thread_id`: \"2\"** → Inicia un **nuevo hilo (thread) de conversación** (nueva sesión).  \n",
    "- **`user_id`: \"1\"** → Mantiene el **mismo ID de usuario**, lo que permite **acceder a la memoria guardada** (memoria a largo plazo).  \n",
    "\n",
    "> **¿Por qué es importante esto?**  \n",
    "Aunque es una **nueva sesión**, el chatbot **recuerda los detalles del usuario** porque el ID de usuario sigue siendo el mismo.\n",
    "\n",
    "#### Proporcionar una Nueva Entrada del Usuario  \n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"Hi! Where would you recommend I drive with my vehicle in my town?\")]\n",
    "```\n",
    "- **Entrada del usuario:** El usuario pregunta por recomendaciones para conducir en su ciudad.  \n",
    "- El chatbot **personalizará su respuesta** basándose en lo que **ya sabe** del usuario.\n",
    "\n",
    "#### Procesar la Entrada con la Memoria Guardada  \n",
    "```python\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "- **Ejecuta el flujo de trabajo del chatbot (graph):**  \n",
    "  1. **Recupera la memoria:** Busca la información almacenada para **user_id = 1**.  \n",
    "  2. **Usa la memoria:** Sabe que el usuario se llama **Julio**, que le gusta **conducir una Vespa** en **San Francisco**.  \n",
    "  3. **Genera una respuesta personalizada:** Recomienda lugares adecuados para **ir en Vespa en San Francisco**.  \n",
    "  4. **Muestra la respuesta en pantalla.**\n",
    "\n",
    "#### ¿Qué sucede internamente?  \n",
    "\n",
    "**Memoria Utilizada:**  \n",
    "- **Nombre:** Julio  \n",
    "- **Le gusta conducir una Vespa**  \n",
    "- **Ubicación:** San Francisco  \n",
    "\n",
    "**Ejemplo de Respuesta del Chatbot:**  \n",
    "> \"¡Hola Julio! Como disfrutas conduciendo una Vespa por San Francisco, te recomendaría explorar las vistas panorámicas a lo largo del Embarcadero o dar un paseo por el Golden Gate Park.\"\n",
    "\n",
    "#### Lecciones Clave  \n",
    "\n",
    "1. **Sesiones Separadas, Misma Memoria:**  \n",
    "   - El chatbot trata esta conversación como una **nueva sesión** (nuevo hilo).  \n",
    "   - Aún así, **recuerda detalles previos** porque usa el **mismo ID de usuario**.\n",
    "\n",
    "2. **Respuestas Personalizadas:**  \n",
    "   - Gracias a la **memoria a largo plazo**, el chatbot puede hacer respuestas **relevantes**, incluso en una nueva sesión.  \n",
    "\n",
    "3. **Aplicaciones Prácticas:**  \n",
    "   - Permite a los usuarios tener **múltiples conversaciones** sin tener que repetir sus detalles personales.  \n",
    "   - Es ideal para chatbots diseñados para manejar **relaciones continuas con los usuarios**.  \n",
    "\n",
    "#### ¿Por qué es importante usar un nuevo `thread_id`?  \n",
    "- **`thread_id = 1`** → Puede haber sido una conversación inicial sobre el usuario.  \n",
    "- **`thread_id = 2`** → Se enfoca en recomendaciones de viaje sin perder el contexto previo.  \n",
    "\n",
    "Este sistema permite que el chatbot **gestione múltiples temas** mientras sigue recordando quién es el usuario, creando una **experiencia fluida y personalizada en todas las sesiones**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00ef0a-aa22-4a8b-a239-a07bd84bda25",
   "metadata": {},
   "source": [
    "## Pero, ¿dónde se guarda esta memoria a largo plazo? Y, ¿persistirá esta memoria a largo plazo si cerramos nuestra aplicación y la volvemos a abrir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600ff19-f8eb-4069-8ced-28f8d90c1d93",
   "metadata": {},
   "source": [
    "#### ¿Dónde se guarda la memoria a largo plazo?\n",
    "   - La memoria a largo plazo se guarda en la implementación de `BaseStore` utilizada para `store`, que en tu caso está especificada como `InMemoryStore()` y se asigna a `across_thread_memory`.\n",
    "\n",
    "   - **Componentes clave para el almacenamiento de memoria**:\n",
    "     - **Espacio de nombres y clave**: La memoria se guarda con un espacio de nombres (`(\"memory\", user_id)`) y una clave (`\"user_memory\"`).\n",
    "     - **Contenido de la memoria**: El contenido de la memoria se almacena como un diccionario, con la clave `{\"memory\": <new_memory_content>}`.\n",
    "\n",
    "#### ¿Cómo es persistente la memoria?\n",
    "   - La persistencia de la memoria depende de la implementación de `BaseStore`. En este caso, `InMemoryStore` es un almacenamiento temporal en memoria que no persiste los datos en disco ni en ningún almacenamiento externo.\n",
    "\n",
    "#### ¿Persistirá la memoria si la aplicación se cierra y se vuelve a abrir?\n",
    "   - No, la memoria **no persistirá** si la aplicación se cierra y se vuelve a abrir, ya que `InMemoryStore` solo mantiene la memoria en la memoria en tiempo de ejecución de la aplicación. Cuando la aplicación se apaga, todos los datos en `InMemoryStore` se pierden.\n",
    "\n",
    "#### ¿Qué es la memoria en tiempo de ejecución (runtime memory) de la aplicación?\n",
    "\n",
    "La memoria en tiempo de ejecución (runtime memory) de la aplicación es como el \"espacio de trabajo\" de tu computadora, donde se almacenan temporalmente todos los datos y procesos necesarios para que una aplicación funcione mientras está abierta. Piensa en ella como una pizarra blanca:\n",
    "\n",
    "1. **Almacenamiento temporal**: La memoria en tiempo de ejecución (runtime memory) se usa para guardar información que la aplicación necesita *en este momento*. Por ejemplo, cuando escribes un mensaje en una aplicación de chat, el texto que escribes se guarda en la memoria en tiempo de ejecución antes de ser enviado.\n",
    "\n",
    "2. **Se borra al cerrar la aplicación**: Al igual que una pizarra blanca que se borra cuando terminas de usarla, la memoria en tiempo de ejecución se limpia cuando la aplicación se cierra o cuando reinicias tu computadora. Por eso, si una aplicación no guarda sus datos en un lugar más permanente (como un archivo o una base de datos), se pierde todo lo almacenado en la memoria en tiempo de ejecución.\n",
    "\n",
    "3. **Depende de la RAM**: La memoria en tiempo de ejecución reside en la **RAM (Memoria de Acceso Aleatorio)** de tu computadora, que es mucho más rápida que el almacenamiento a largo plazo (como el disco duro), pero también es temporal.\n",
    "\n",
    "#### Ejemplo\n",
    "Imagina que estás usando una aplicación de notas. Si escribes algo pero no lo guardas y luego cierras la aplicación, el texto desaparecerá para siempre. Esto sucede porque el texto solo estaba almacenado en la memoria en tiempo de ejecución (runtime memory) y no se guardó en un almacenamiento permanente (como el disco duro de tu computadora). Si presionas \"Guardar\", el texto se escribe en un almacenamiento permanente, por lo que podrás abrirlo más tarde.\n",
    "\n",
    "#### Hacer que la memoria sea persistente entre sesiones\n",
    "Para que la memoria a largo plazo en el código de LangGraph proporcionado sea persistente y se guarde en tu disco duro, tendrías que reemplazar `InMemoryStore` (que solo mantiene los datos en la memoria en tiempo de ejecución) con una alternativa que escriba en un almacenamiento persistente, como una base de datos SQLite, un archivo o alguna otra forma de almacenamiento a largo plazo. **Este es todavía un campo muy nuevo en LangGraph.** Añadiremos más información sobre esto en el bootcamp en cuanto veamos una implementación sólida en la documentación de LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816085d-e823-4d84-aa7e-091420e5d758",
   "metadata": {},
   "source": [
    "## Y, con eso, podríamos continuar nuestra conversación  \n",
    "* Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f90956a-2ea9-4cd3-b4f3-a00df110e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Great, are there any Whole Foods stores nearby that I can check out? I like a health food after driving.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Absolutely, Julio! There are several Whole Foods locations in San Francisco where you can grab some healthy food after your Vespa rides. Here are a few options:\n",
      "\n",
      "1. **Whole Foods Market on Market Street** - Located at 399 4th Street, it's right in the SoMa area and quite accessible.\n",
      "2. **Whole Foods Market on California Street** - Situated at 1765 California Street, this one is in the Pacific Heights neighborhood.\n",
      "3. **Whole Foods Market on Haight Street** - Found at 690 Stanyan Street, it's near Golden Gate Park, which could be convenient if you're riding around there.\n",
      "\n",
      "These stores should have a great selection of health foods for you to enjoy. Safe travels!\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Great, are there any Whole Foods stores nearby that I can check out? I like a health food after driving.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123b580-a27c-4e89-ae1b-69e2d3fee1c3",
   "metadata": {},
   "source": [
    "## Cómo ejecutar el código desde Visual Studio Code  \n",
    "* En Visual Studio Code, abre el archivo `025-store.py`  \n",
    "* En la terminal, asegúrate de estar en el directorio del archivo y ejecuta:  \n",
    "    * `python 025-store.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7c228-7a33-47c2-ac1b-4a1a27af0be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
